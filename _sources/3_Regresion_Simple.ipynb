{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b4077af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelo de Regresión Lineal\n",
    "\n",
    "La regresion es un método que nos permite estudiar la relación entre una variable de resultado $Y$ y una covariable o predictor $X$.\n",
    "\n",
    "La función de regresión, $r(X)$, resume la relación entre $X$ y $Y$: \n",
    "\n",
    "$$\n",
    "r(x) = \\mathbb{E}(Y | X = x) = \\int y f(y|x) dy.\n",
    "$$ \n",
    "\n",
    "Nuestro objetivo es estimar $r(x)$ usando datos de la forma: \n",
    "\n",
    "$$\n",
    "(Y_1, X_1), ..., (Y_n, X_n) \\sim F_{X, Y}.\n",
    "$$\n",
    "\n",
    "Definamos el error $\\varepsilon = Y - r(x)$. De esta manera, podemos escribir: \n",
    "\n",
    "$$\n",
    "Y = r(x) + \\varepsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c567e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión Simple o Univariada\n",
    "\n",
    "En su versión más simple, $X$ es unidimensional y asumimos que $r(x)$ es lineal. Así, \n",
    "\n",
    "$$r(x) = \\beta_0 + \\beta_1 x.$$\n",
    "\n",
    "Inicialmente también asumiremos que $\\mathbb{V}(\\varepsilon | X = x) = \\sigma^2$ no depende de $X$. \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "<b>EJEMPLO:</b>\n",
    "<p>\n",
    "\n",
    "La función de demanda inversa establece la relación entre precios $P_i$ y cantidades $Q_i$. Esta relación la podemos estimar a partir de una modelo de la forma: \n",
    "    \n",
    "$$P_i = \\alpha + \\beta \\cdot Q_i + u_i$$\n",
    "\n",
    "Quisieramos probar la siguiente hipótesis:\n",
    "    \n",
    "$$H_0: \\beta < 0$$\n",
    "    \n",
    "$$H_1: \\beta \\geq 0$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ae778",
   "metadata": {},
   "source": [
    "El modelo regresión lineal simple se define como: \n",
    "\n",
    "$$\n",
    "Y_i = r(x) + \\varepsilon_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i\n",
    "$$ \n",
    "\n",
    "$$\\mathbb{E}(\\varepsilon_i | X_i) = 0$$\n",
    "\n",
    "$$\\mathbb{V}(\\varepsilon_i | X_i) = \\sigma^2$$\n",
    "\n",
    "Observe que los parámetros desconocidos en este modelo son el intercepto $\\beta_0$, la pendiente $\\beta_1$, y la varianza $\\sigma^2$.\n",
    "\n",
    "Note que a partir de los supuestos anteriores podemos determinar que:\n",
    "\n",
    "$$\\mathbb{E}(Y_i | X_i) = \\mathbb{E}(\\beta_0 + \\beta_1 X_i | X_i) = \\beta_0 + \\beta_1 X_i$$\n",
    "\n",
    "$$\\mathbb{V}(Y_i | X_i) = \\mathbb{V}(\\varepsilon_i | X_i) = \\sigma^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd39140",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mínimos Cuadrados Ordinarios (MCO)\n",
    "\n",
    "Para estimar $\\beta_0$ y $\\beta_1$ podemos usar el método MCO, de manera formal: \n",
    "\n",
    "$$\\{\\hat\\beta_0, \\hat\\beta_1\\} = \\operatorname*{arg\\,min} \\mathcal{L}(\\beta_0, \\beta_1)$$\n",
    "\n",
    "donde $\\mathcal{L}(\\beta_0, \\beta_1) = \\sum_i {\\varepsilon_i}^2 = \\sum_i ({Y_i} - \\beta_0 - \\beta_1 X_i)^2$\n",
    "\n",
    "Las condiciones de primer orden (CPO) de este problema estan dadas por: \n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial\\beta_0} =  \\sum_i (-2) \\cdot ({Y_i} - \\beta_0 - \\beta_1 X_i) = 0$$\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial\\beta_1} =  \\sum_i (-2 X_i) \\cdot ({Y_i} - \\beta_0 - \\beta_1 X_i) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8001c4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A partir de estas condiciones obtenemos los siguientes **estimadores**:\n",
    "\n",
    "$$\\hat\\beta_0 = \\overline Y - \\hat\\beta_1 \\overline X$$\n",
    "\n",
    "$$\\hat\\beta_1 = \\frac{\\sum_i (X_i - \\overline X)(Y_i - \\overline Y)}{\\sum_i (X_i - \\overline X)^2} = \\frac{\\mathbb{\\hat Cov}(X, Y)}{\\mathbb{\\hat V}(X)}$$ \n",
    "\n",
    "De esta manera, la **linea ajustada** está dada por $\\hat r(x) = \\hat\\beta_0 + \\hat\\beta_1 x$, y los **valores predichos** se definen como $\\hat Y_i = \\hat r(X_i)$\n",
    "\n",
    "Definimos además el **residual** como $$\\hat \\varepsilon_i = Y_i - \\hat Y_i = Y_i - \\left(\\hat\\beta_0 + \\hat\\beta_1 X_i \\right)$$\n",
    "\n",
    "Un estimador insesgado de $\\sigma^2$ es \n",
    "\n",
    "$$\\hat \\sigma^2 = \\left(\\frac{1}{n-2} \\right) \\sum_{i = 1}^{n} {\\hat\\varepsilon_i}^2$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
