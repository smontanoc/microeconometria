
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Parte II - Regresión Lineal &#8212; Microeconometria Aplicada</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Modulo_2';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Parte I - Fundamentos" href="Modulo_1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Microeconometria Aplicada - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Microeconometria Aplicada - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Microeconometría Aplicada
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Modulo_1.html">Parte I - Fundamentos</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Parte II - Regresión Lineal</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FModulo_2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Modulo_2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Parte II - Regresión Lineal</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-aprendizaje-y-tamano-de-clase">2.1 Aplicación: Aprendizaje y Tamaño de Clase</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-simple-o-univariada">2.2 Regresión Simple o Univariada</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supuestos-del-modelo">2.3 Supuestos del Modelo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linealidad-en-parametros">2.3.1 Linealidad en Parámetros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-multicolinealidad-o-rango-completo">2.3.2 No Multicolinealidad o Rango Completo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homoscedasticidad">2.3.3 Homoscedasticidad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-autocorrelacion">2.3.4 No Autocorrelación</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribucion-normal-de-los-errores">2.3.5 Distribución Normal de los Errores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independencia-condicional-o-exogenidad">2.3.6 Independencia Condicional o Exogenidad</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimacion-por-mco">2.4 Estimación por MCO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-relacion-entre-aprendizaje-y-crecimiento">2.5 Aplicacion : Relación entre Aprendizaje y Crecimiento</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residuales-de-la-regresion-e-i-y-i-hat-y-i">2.6 Residuales de la Regresión: <span class="math notranslate nohighlight">\(e_i = Y_i - \hat Y_i\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propiedades-de-los-estimadores-hat-beta-k">2.7 Propiedades de los estimadores <span class="math notranslate nohighlight">\(\hat\beta_k\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inferencia-en-regresion-simple">2.8 Inferencia en Regresión Simple</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruebas-de-hipotesis-sobre-hat-beta-k">2.8.1 Pruebas de Hipótesis sobre <span class="math notranslate nohighlight">\(\hat\beta_k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#varianza-de-hat-beta-k">2.8.2 Varianza de <span class="math notranslate nohighlight">\(\hat\beta_k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#varianza-de-los-errores">2.8.3 Varianza de los Errores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruebas-de-hipotesis-individuales">2.8.4 Pruebas de Hipótesis Individuales</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intervalo-de-confianza-para-beta-k">2.8.4 Intervalo de confianza para <span class="math notranslate nohighlight">\(\beta_k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coeficiente-de-determinacion-r-2">2.8.5 Coeficiente de Determinación: <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prueba-de-significancia-global">2.8.6 Prueba de Significancia Global</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion-e-inferencia-para-hat-y-i">2.8.7 Predicción e Inferencia para <span class="math notranslate nohighlight">\(\hat Y_i\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion-media">2.8.8 Predicción Media</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion-individual">2.8.9 Predicción Individual</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimacion-por-maxima-verosimilitud">2.9 Estimación por Máxima Verosimilitud</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-de-regresion-multiple">2.10 Modelo de Regresión Multiple</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-en-forma-matricial">2.10.1 Modelo en Forma Matricial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supuestos-en-forma-matricial">2.10.2 Supuestos en forma Matricial</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimos-cuadrados-ordinarios">2.11 Mínimos Cuadrados Ordinarios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimacion-de-beta">2.12 Estimación de <span class="math notranslate nohighlight">\(\beta\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-y-sumatorias">2.12.1 Matrices y Sumatorias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo">2.12.2 Ejemplo</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propiedades-de-hat-beta">2.13 Propiedades de <span class="math notranslate nohighlight">\(\hat\beta\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matriz-de-varianza-y-covarianza">2.14 Matriz de Varianza y Covarianza</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pruebas-de-significancia-individual">2.15 Pruebas de Significancia Individual</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bondad-de-ajuste-r-2-y-r-2-ajustado">2.16 Bondad de Ajuste: <span class="math notranslate nohighlight">\(R^2\)</span> y <span class="math notranslate nohighlight">\(R^2\)</span>-Ajustado</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prueba-de-signifiancia-global">2.17 Prueba de Signifiancia Global</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#otras-pruebas-de-hipotesis">2.18 Otras Pruebas de Hipotesis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estadistico-t">2.18.1 Estadístico T</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimos-cuadrados-restringidos">2.19 Minimos Cuadrados Restringidos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.19.1 Ejemplo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-variables">2.19.2 Selección de Variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variables-binarias-y-categoricas">2.20 Variables Binarias y Categoricas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion">2.21 Aplicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interacciones">2.22 Interacciones</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaccion-entre-variables-binarias">2.22.1 Interacción entre Variables Binarias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaccion-entre-una-variable-binaria-y-una-continua">2.22.2 Interacción entre una Variable Binaria y una Continua</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-de-gauss-markov">2.23 Teorema de Gauss-Markov</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#demostacion-del-teorema-de-gauss-markov">2.23.1 Demostación del Teorema de Gauss-Markov</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="parte-ii-regresion-lineal">
<h1>Parte II - Regresión Lineal<a class="headerlink" href="#parte-ii-regresion-lineal" title="Link to this heading">#</a></h1>
<p>La regresión es un método que nos permite estudiar la relación entre una variable de resultado <span class="math notranslate nohighlight">\(Y\)</span> y una covariable o predictor <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Aunque no siempre se busca determinar si existe una relación causal, el problema de causalidad ocupa un lugar central en la agenda de los microeconometristas.</p>
<section id="aplicacion-aprendizaje-y-tamano-de-clase">
<h2>2.1 Aplicación: Aprendizaje y Tamaño de Clase<a class="headerlink" href="#aplicacion-aprendizaje-y-tamano-de-clase" title="Link to this heading">#</a></h2>
<p>¿Cuál es la mejor forma de asignar el gasto en educación? ¿Es buena idea tener clases de menor tamaño? Estas son preguntas de interés no solo para economistas sino también para gobiernos y la sociedad en general. Sin embargo, por mucho tiempo la literatura presentó evidencia no concluyente o mixta.</p>
<p>Parte del problema para responder estas preguntas está relacionado con que sabemos muy poco acerca de cómo aprenden los estudiantes. En particular, múltiples factores afectan los resultados de los estudiantes. Suponiendo que existe una función de producción educativa, <span class="math notranslate nohighlight">\(f\)</span>, ¿qué elementos cree usted que explican el aprendizaje?</p>
<div class="amsmath math notranslate nohighlight" id="equation-f1ee36f5-9f67-4bae-8ca1-ec870bcdce2a">
<span class="eqno">(1)<a class="headerlink" href="#equation-f1ee36f5-9f67-4bae-8ca1-ec870bcdce2a" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Aprendizaje} = f(?)
\end{equation}\]</div>
<p>Existen diferentes factores que confluyen al tiempo en el aprendizaje de los estudiantes, entre ellos</p>
<ul class="simple">
<li><p>Calidad docente</p></li>
<li><p>Compañeros de clase</p></li>
<li><p>Padres y condiciones del hogar</p></li>
<li><p>Infraestructura</p></li>
<li><p>Tamaño de clase</p></li>
<li><p>Habilidades innatas</p></li>
</ul>
<p><a class="reference external" href="https://doi.org/10.1162/003355399556052">Krueger (1999)</a> utilizó información de un experimento aleatario en Tennessee (STAR), encontrando que los estudiantes asignados a clases más pequeñas obtienen mejores resultados en pruebas estandarizadas y que este efecto positivo es persistente en el tiempo. Luego veremos por qué las estimaciones de Krueger pueden ser interpretadas como relaciones causales.</p>
</section>
<section id="regresion-simple-o-univariada">
<h2>2.2 Regresión Simple o Univariada<a class="headerlink" href="#regresion-simple-o-univariada" title="Link to this heading">#</a></h2>
<p>La función de regresión, <span class="math notranslate nohighlight">\(r(X)\)</span>, resume la relación entre <span class="math notranslate nohighlight">\(X\)</span> y <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
r(x) = \mathbb{E}(Y | X = x) = \int y f(y|x) dy.
\]</div>
<p>Nuestro objetivo es estimar <span class="math notranslate nohighlight">\(r(x)\)</span> usando datos de la forma</p>
<div class="math notranslate nohighlight">
\[\{Y_i , X_i\}_{i = 1}^n\]</div>
<p>Definamos el error <span class="math notranslate nohighlight">\(\varepsilon = Y - r(x)\)</span>. De esta manera, podemos escribir:</p>
<div class="math notranslate nohighlight">
\[
Y = r(x) + \varepsilon
\]</div>
<p>En su versión más simple, <span class="math notranslate nohighlight">\(X\)</span> es unidimensional y asumimos que <span class="math notranslate nohighlight">\(r(x)\)</span> es lineal. Así,</p>
<div class="math notranslate nohighlight">
\[r(x) = \beta_0 + \beta_1 x.\]</div>
<p>El modelo regresión lineal simple se define como:</p>
<div class="math notranslate nohighlight">
\[
Y_i = r(x) + \varepsilon_i = \beta_0 + \beta_1 X_i + \varepsilon_i
\]</div>
<p>Observe que los parámetros desconocidos en este modelo son el intercepto <span class="math notranslate nohighlight">\(\beta_0\)</span> y la pendiente <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
</section>
<section id="supuestos-del-modelo">
<h2>2.3 Supuestos del Modelo<a class="headerlink" href="#supuestos-del-modelo" title="Link to this heading">#</a></h2>
<p>Para estimar el modelo de regresión suponemos:</p>
<ul class="simple">
<li><p>Linealidad en los parámetros</p></li>
<li><p>No multicolinealidad o Rango Completo</p></li>
<li><p>Homoscedasticidad</p></li>
<li><p>No autocorrelación</p></li>
<li><p>Distribución Normal de los Errores</p></li>
<li><p>Independencia Condicional o Exogenidad</p></li>
</ul>
<p>Algunos de estos supuestos hace que el modelo sea <strong>estimable</strong>, otros nos permiten hacer <strong>inferencia</strong> sobre los parámetros</p>
<section id="linealidad-en-parametros">
<h3>2.3.1 Linealidad en Parámetros<a class="headerlink" href="#linealidad-en-parametros" title="Link to this heading">#</a></h3>
<p>Este supuesto es necesario para que el modelo sea <strong>estimable</strong>.</p>
<p>Se refiere a la forma funcional de la función de regresión <span class="math notranslate nohighlight">\(r(x) = \beta_0 + \beta_1 X_i\)</span>.</p>
<p>Note que nos interesa estimar <span class="math notranslate nohighlight">\(\beta_k\)</span>, no <span class="math notranslate nohighlight">\(\beta_k^2\)</span>, <span class="math notranslate nohighlight">\(\ln(\beta_k)\)</span>, o cualquier otra función <span class="math notranslate nohighlight">\(g(\beta_k)\)</span>.</p>
<div class="alert alert-block alert-warning"> 
<b>EJEMPLO:</b>
<p>
<p>Suponga queremos estimar el parámetro <span class="math notranslate nohighlight">\(\beta\)</span> de la función <span class="math notranslate nohighlight">\(y = A X^\beta e^\varepsilon\)</span>. ¿Podemos usar el modelo de regresión lineal?</p>
<p>Sí, note que podemos utilizar la siguiente transformación:</p>
<div class="math notranslate nohighlight">
\[\ln y_i = \alpha + \beta \cdot X_i + \varepsilon_i, \text{ con } \alpha = \ln A\]</div>
<p>¿Qué pasa si <span class="math notranslate nohighlight">\(y = A X^\beta e^\varepsilon + u\)</span> ?</p>
</div></section>
<section id="no-multicolinealidad-o-rango-completo">
<h3>2.3.2 No Multicolinealidad o Rango Completo<a class="headerlink" href="#no-multicolinealidad-o-rango-completo" title="Link to this heading">#</a></h3>
<p>Este supuesto es necesario para que el modelo sea <strong>estimable</strong>.</p>
<p>El supuesto hace referencia a la información que aportan diferentes covariables. En primer lugar, este supuesto señala que:</p>
<div class="math notranslate nohighlight">
\[|\rho(X_{1i}, X_{2i})| \neq 1\]</div>
<p>Donde <span class="math notranslate nohighlight">\(\rho\)</span> es el coeficiente de correlación. Este supuesto será más relevante cuando estudiemos el modelo de regresión múltiple.</p>
<p>Adicionalmente, es necesario que el número de observaciones en los datos <span class="math notranslate nohighlight">\(n\)</span> sea mayor al número de parámetros <span class="math notranslate nohighlight">\(k\)</span> que queremos estimar. Es decir</p>
<div class="math notranslate nohighlight">
\[n \geq k\]</div>
</section>
<section id="homoscedasticidad">
<h3>2.3.3 Homoscedasticidad<a class="headerlink" href="#homoscedasticidad" title="Link to this heading">#</a></h3>
<p>Este supuesto es sobre la varianza y nos facilita realizar pruebas de hipótesis</p>
<p>El supuesto establece que:</p>
<div class="math notranslate nohighlight">
\[V(\varepsilon_i | X) = \sigma^2\]</div>
<p>La violación de este supuesto se conoce como <strong>heteroscedasticidad</strong>. En un modelo <strong>heteroscedástico</strong></p>
<div class="math notranslate nohighlight">
\[V(\varepsilon_i | X) = \sigma_i^2\]</div>
</section>
<section id="no-autocorrelacion">
<h3>2.3.4 No Autocorrelación<a class="headerlink" href="#no-autocorrelacion" title="Link to this heading">#</a></h3>
<p>Este supuesto establece que:</p>
<div class="math notranslate nohighlight">
\[Cov(\varepsilon_i, \varepsilon_j | X) = 0 \text{ para todo } i \neq j\]</div>
<p>La violación de este supuesto se conoce como <strong>autocorrelación</strong>.</p>
</section>
<section id="distribucion-normal-de-los-errores">
<h3>2.3.5 Distribución Normal de los Errores<a class="headerlink" href="#distribucion-normal-de-los-errores" title="Link to this heading">#</a></h3>
<p>Este supuesto se hace sobre los errores entonces también afecta las pruebas de hipótesis.</p>
<p>El supuesto señala que:</p>
<div class="math notranslate nohighlight">
\[\varepsilon_i | X \sim N(0, \sigma^2)\]</div>
<p>Observe que por el teorema del límite central este supuesto no es tan fuerte, y si muy útil para determinar cómo se distribuyen ciertos estadísticos.</p>
</section>
<section id="independencia-condicional-o-exogenidad">
<h3>2.3.6 Independencia Condicional o Exogenidad<a class="headerlink" href="#independencia-condicional-o-exogenidad" title="Link to this heading">#</a></h3>
<p>Para que los estimadores de <span class="math notranslate nohighlight">\(\beta_k\)</span> sean insesgados y consistentes suponemos que</p>
<div class="math notranslate nohighlight">
\[E(\varepsilon_i | X) = E(\varepsilon_i)\]</div>
<p>Observe que combinado con el supuesto anterior tenemos que:</p>
<div class="math notranslate nohighlight">
\[E(\varepsilon_i | X) = 0\]</div>
<p>Según este supuesto <span class="math notranslate nohighlight">\(\varepsilon\)</span> es indepente de <span class="math notranslate nohighlight">\(X\)</span>. En otras palabras <span class="math notranslate nohighlight">\(X\)</span> es una variable exogena.</p>
<p>Este supuesto garantiza que los efectos encontrados pueden se interpretados como estimaciones <strong>causales</strong>.</p>
</section>
</section>
<section id="estimacion-por-mco">
<h2>2.4 Estimación por MCO<a class="headerlink" href="#estimacion-por-mco" title="Link to this heading">#</a></h2>
<p>Para estimar <span class="math notranslate nohighlight">\(\beta_0\)</span> y <span class="math notranslate nohighlight">\(\beta_1\)</span> podemos usar el método de Mínimos Cuadrados Ordinarios (MCO), de manera formal:</p>
<div class="math notranslate nohighlight">
\[\{\hat\beta_0, \hat\beta_1\} = \operatorname*{arg\,min} \mathcal{L}(\beta_0, \beta_1)\]</div>
<p>donde <span class="math notranslate nohighlight">\(\mathcal{L}(\beta_0, \beta_1) = \sum_i {\varepsilon_i}^2 = \sum_i ({Y_i} - \beta_0 - \beta_1 X_i)^2\)</span></p>
<p>Las condiciones de primer orden (CPO) de este problema estan dadas por:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial\mathcal{L}}{\partial\beta_0} \bigg|_{(\hat\beta_0, \hat\beta_1)} = \sum_i (-2) \cdot ({Y_i} - \beta_0 - \beta_1 X_i) \bigg|_{(\hat\beta_0, \hat\beta_1)} = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial\mathcal{L}}{\partial\beta_1} \bigg|_{(\hat\beta_0, \hat\beta_1)} = \sum_i (-2 X_i) \cdot ({Y_i} - \beta_0 - \beta_1 X_i) \bigg|_{(\hat\beta_0, \hat\beta_1)} = 0\]</div>
<p>Estas ecuaciones se conocen como <strong>ecuaciones normales</strong>.</p>
<p>A partir de estas condiciones obtenemos los siguientes <strong>estimadores</strong>:</p>
<div class="math notranslate nohighlight">
\[\hat\beta_0 = \overline Y - \hat\beta_1 \overline X\]</div>
<div class="math notranslate nohighlight">
\[\hat\beta_1 = \frac{\sum_i (X_i - \overline X)(Y_i - \overline Y)}{\sum_i (X_i - \overline X)^2} = \frac{\mathbb{\hat Cov}(X, Y)}{\mathbb{\hat V}(X)}\]</div>
<p>De esta manera, la <strong>linea ajustada</strong> está dada por <span class="math notranslate nohighlight">\(\hat r(x) = \hat\beta_0 + \hat\beta_1 x\)</span>, y los <strong>valores predichos</strong> se definen como <span class="math notranslate nohighlight">\(\hat Y_i = \hat r(X_i)\)</span></p>
<p>Definimos además el <strong>residual</strong> como $<span class="math notranslate nohighlight">\(e_i = Y_i - \hat Y_i = Y_i - \left(\hat\beta_0 + \hat\beta_1 X_i \right)\)</span>$</p>
</section>
<section id="aplicacion-relacion-entre-aprendizaje-y-crecimiento">
<h2>2.5 Aplicacion : Relación entre Aprendizaje y Crecimiento<a class="headerlink" href="#aplicacion-relacion-entre-aprendizaje-y-crecimiento" title="Link to this heading">#</a></h2>
<p>Vamos a usar datos de las pruebas PISA, que toman estudiantes de 15 años de diversos paises, para estudiar la relación entre aprendizaje y recursos económicos</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">datos</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span>
<span class="w">  </span><span class="n">cnt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;ALB&quot;</span><span class="p">,</span><span class="s">&quot;ARE&quot;</span><span class="p">,</span><span class="s">&quot;ARG&quot;</span><span class="p">,</span><span class="s">&quot;AUS&quot;</span><span class="p">,</span><span class="s">&quot;AUT&quot;</span><span class="p">,</span><span class="s">&quot;BEL&quot;</span><span class="p">,</span><span class="s">&quot;BGR&quot;</span><span class="p">,</span><span class="s">&quot;BRA&quot;</span><span class="p">,</span><span class="s">&quot;BRN&quot;</span><span class="p">,</span><span class="s">&quot;CAN&quot;</span><span class="p">,</span><span class="s">&quot;CHE&quot;</span><span class="p">,</span><span class="s">&quot;CHL&quot;</span><span class="p">,</span><span class="s">&quot;COL&quot;</span><span class="p">,</span><span class="s">&quot;CRI&quot;</span><span class="p">,</span><span class="s">&quot;CZE&quot;</span><span class="p">,</span><span class="s">&quot;DEU&quot;</span><span class="p">,</span><span class="s">&quot;DNK&quot;</span><span class="p">,</span><span class="s">&quot;DOM&quot;</span><span class="p">,</span><span class="s">&quot;ESP&quot;</span><span class="p">,</span><span class="s">&quot;EST&quot;</span><span class="p">,</span><span class="s">&quot;FIN&quot;</span><span class="p">,</span><span class="s">&quot;FRA&quot;</span><span class="p">,</span><span class="s">&quot;GBR&quot;</span><span class="p">,</span><span class="s">&quot;GEO&quot;</span><span class="p">,</span><span class="s">&quot;GRC&quot;</span><span class="p">,</span><span class="s">&quot;GTM&quot;</span><span class="p">,</span><span class="s">&quot;HKG&quot;</span><span class="p">,</span><span class="s">&quot;HRV&quot;</span><span class="p">,</span><span class="s">&quot;HUN&quot;</span><span class="p">,</span><span class="s">&quot;IDN&quot;</span><span class="p">,</span><span class="s">&quot;IRL&quot;</span><span class="p">,</span><span class="s">&quot;ISL&quot;</span><span class="p">,</span><span class="s">&quot;ISR&quot;</span><span class="p">,</span><span class="s">&quot;ITA&quot;</span><span class="p">,</span><span class="s">&quot;JAM&quot;</span><span class="p">,</span><span class="s">&quot;JOR&quot;</span><span class="p">,</span><span class="s">&quot;JPN&quot;</span><span class="p">,</span><span class="s">&quot;KAZ&quot;</span><span class="p">,</span><span class="s">&quot;KHM&quot;</span><span class="p">,</span><span class="s">&quot;KOR&quot;</span><span class="p">,</span><span class="s">&quot;KSV&quot;</span><span class="p">,</span><span class="s">&quot;LTU&quot;</span><span class="p">,</span><span class="s">&quot;LVA&quot;</span><span class="p">,</span><span class="s">&quot;MAC&quot;</span><span class="p">,</span><span class="s">&quot;MAR&quot;</span><span class="p">,</span><span class="s">&quot;MDA&quot;</span><span class="p">,</span><span class="s">&quot;MEX&quot;</span><span class="p">,</span><span class="s">&quot;MKD&quot;</span><span class="p">,</span><span class="s">&quot;MLT&quot;</span><span class="p">,</span><span class="s">&quot;MNE&quot;</span><span class="p">,</span><span class="s">&quot;MNG&quot;</span><span class="p">,</span><span class="s">&quot;MYS&quot;</span><span class="p">,</span><span class="s">&quot;NLD&quot;</span><span class="p">,</span><span class="s">&quot;NOR&quot;</span><span class="p">,</span><span class="s">&quot;NZL&quot;</span><span class="p">,</span><span class="s">&quot;PAN&quot;</span><span class="p">,</span><span class="s">&quot;PER&quot;</span><span class="p">,</span><span class="s">&quot;PHL&quot;</span><span class="p">,</span><span class="s">&quot;POL&quot;</span><span class="p">,</span><span class="s">&quot;PRT&quot;</span><span class="p">,</span><span class="s">&quot;PRY&quot;</span><span class="p">,</span><span class="s">&quot;PSE&quot;</span><span class="p">,</span><span class="s">&quot;QAT&quot;</span><span class="p">,</span><span class="s">&quot;QAZ&quot;</span><span class="p">,</span><span class="s">&quot;QUR&quot;</span><span class="p">,</span><span class="s">&quot;ROU&quot;</span><span class="p">,</span><span class="s">&quot;SAU&quot;</span><span class="p">,</span><span class="s">&quot;SGP&quot;</span><span class="p">,</span><span class="s">&quot;SLV&quot;</span><span class="p">,</span><span class="s">&quot;SRB&quot;</span><span class="p">,</span><span class="s">&quot;SVK&quot;</span><span class="p">,</span><span class="s">&quot;SVN&quot;</span><span class="p">,</span><span class="s">&quot;SWE&quot;</span><span class="p">,</span><span class="s">&quot;TAP&quot;</span><span class="p">,</span><span class="s">&quot;THA&quot;</span><span class="p">,</span><span class="s">&quot;TUR&quot;</span><span class="p">,</span><span class="s">&quot;URY&quot;</span><span class="p">,</span><span class="s">&quot;USA&quot;</span><span class="p">,</span><span class="s">&quot;UZB&quot;</span><span class="p">,</span><span class="s">&quot;VNM&quot;</span><span class="p">),</span>
<span class="w">  </span><span class="n">math</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">368.25405</span><span class="p">,</span><span class="m">433.82986</span><span class="p">,</span><span class="m">388.54898</span><span class="p">,</span><span class="m">487.16594</span><span class="p">,</span><span class="m">490.87702</span><span class="p">,</span><span class="m">494.01554</span><span class="p">,</span><span class="m">418.33838</span><span class="p">,</span><span class="m">381.13655</span><span class="p">,</span><span class="m">440.66541</span><span class="p">,</span><span class="m">484.52203</span><span class="p">,</span><span class="m">506.45947</span><span class="p">,</span><span class="m">428.55602</span><span class="p">,</span><span class="m">390.80082</span><span class="p">,</span><span class="m">384.34594</span><span class="p">,</span><span class="m">499.37027</span><span class="p">,</span><span class="m">477.74352</span><span class="p">,</span><span class="m">478.7986</span><span class="p">,</span><span class="m">339.82878</span><span class="p">,</span><span class="m">481.86548</span><span class="p">,</span><span class="m">512.71336</span><span class="p">,</span><span class="m">475.32395</span><span class="p">,</span><span class="m">468.24401</span><span class="p">,</span><span class="m">481.82223</span><span class="p">,</span><span class="m">390.46145</span><span class="p">,</span><span class="m">433.54079</span><span class="p">,</span><span class="m">345.55713</span><span class="p">,</span><span class="m">546.03005</span><span class="p">,</span><span class="m">462.98476</span><span class="p">,</span><span class="m">479.72656</span><span class="p">,</span><span class="m">379.03128</span><span class="p">,</span><span class="m">492.17959</span><span class="p">,</span><span class="m">459.27331</span><span class="p">,</span><span class="m">457.12382</span><span class="p">,</span><span class="m">474.03372</span><span class="p">,</span><span class="m">371.40991</span><span class="p">,</span><span class="m">360.30719</span><span class="p">,</span><span class="m">534.92948</span><span class="p">,</span><span class="m">446.01273</span><span class="p">,</span><span class="m">327.40929</span><span class="p">,</span><span class="m">531.09303</span><span class="p">,</span><span class="m">352.41048</span><span class="p">,</span><span class="m">471.54378</span><span class="p">,</span><span class="m">482.58027</span><span class="p">,</span><span class="m">552.05427</span><span class="p">,</span><span class="m">363.36233</span><span class="p">,</span><span class="m">415.52394</span><span class="p">,</span><span class="m">395.40581</span><span class="p">,</span><span class="m">390.49863</span><span class="p">,</span><span class="m">468.9976</span><span class="p">,</span><span class="m">406.00521</span><span class="p">,</span><span class="m">423.92532</span><span class="p">,</span><span class="m">409.49151</span><span class="p">,</span><span class="m">491.04093</span><span class="p">,</span><span class="m">468.47831</span><span class="p">,</span><span class="m">478.87437</span><span class="p">,</span><span class="m">353.49833</span><span class="p">,</span><span class="m">394.15257</span><span class="p">,</span><span class="m">354.16683</span><span class="p">,</span><span class="m">494.4546</span><span class="p">,</span><span class="m">475.57748</span><span class="p">,</span><span class="m">340.78028</span><span class="p">,</span><span class="m">364.66173</span><span class="p">,</span><span class="m">411.98931</span><span class="p">,</span><span class="m">398.33636</span><span class="p">,</span><span class="m">444.34014</span><span class="p">,</span><span class="m">435.55027</span><span class="p">,</span><span class="m">389.20505</span><span class="p">,</span><span class="m">573.98391</span><span class="p">,</span><span class="m">345.12158</span><span class="p">,</span><span class="m">439.81693</span><span class="p">,</span><span class="m">468.62039</span><span class="p">,</span><span class="m">471.70524</span><span class="p">,</span><span class="m">482.4206</span><span class="p">,</span><span class="m">533.87613</span><span class="p">,</span><span class="m">414.58713</span><span class="p">,</span><span class="m">451.8851</span><span class="p">,</span><span class="m">409.28728</span><span class="p">,</span><span class="m">462.80969</span><span class="p">,</span><span class="m">363.90947</span><span class="p">,</span><span class="m">468.83368</span><span class="p">),</span>
<span class="w">  </span><span class="n">read</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">358.8281</span><span class="p">,</span><span class="m">419.71994</span><span class="p">,</span><span class="m">412.83373</span><span class="p">,</span><span class="m">498.79545</span><span class="p">,</span><span class="m">484.0255</span><span class="p">,</span><span class="m">481.7429</span><span class="p">,</span><span class="m">405.41276</span><span class="p">,</span><span class="m">413.32812</span><span class="p">,</span><span class="m">427.66587</span><span class="p">,</span><span class="m">490.52696</span><span class="p">,</span><span class="m">481.54308</span><span class="p">,</span><span class="m">465.12644</span><span class="p">,</span><span class="m">419.17929</span><span class="p">,</span><span class="m">415.22553</span><span class="p">,</span><span class="m">500.25072</span><span class="p">,</span><span class="m">483.0404</span><span class="p">,</span><span class="m">477.21655</span><span class="p">,</span><span class="m">353.15524</span><span class="p">,</span><span class="m">480.86845</span><span class="p">,</span><span class="m">513.84343</span><span class="p">,</span><span class="m">477.53438</span><span class="p">,</span><span class="m">467.01198</span><span class="p">,</span><span class="m">490.44853</span><span class="p">,</span><span class="m">375.03115</span><span class="p">,</span><span class="m">442.52392</span><span class="p">,</span><span class="m">375.55029</span><span class="p">,</span><span class="m">504.42272</span><span class="p">,</span><span class="m">475.74213</span><span class="p">,</span><span class="m">479.98543</span><span class="p">,</span><span class="m">372.90122</span><span class="p">,</span><span class="m">516.88935</span><span class="p">,</span><span class="m">436.02972</span><span class="p">,</span><span class="m">473.09826</span><span class="p">,</span><span class="m">480.65205</span><span class="p">,</span><span class="m">400.32092</span><span class="p">,</span><span class="m">341.46512</span><span class="p">,</span><span class="m">515.07793</span><span class="p">,</span><span class="m">404.15901</span><span class="p">,</span><span class="m">321.2015</span><span class="p">,</span><span class="m">517.97604</span><span class="p">,</span><span class="m">338.17407</span><span class="p">,</span><span class="m">466.72213</span><span class="p">,</span><span class="m">474.2778</span><span class="p">,</span><span class="m">510.36214</span><span class="p">,</span><span class="m">337.91559</span><span class="p">,</span><span class="m">412.49955</span><span class="p">,</span><span class="m">415.23693</span><span class="p">,</span><span class="m">360.23238</span><span class="p">,</span><span class="m">448.79677</span><span class="p">,</span><span class="m">405.80333</span><span class="p">,</span><span class="m">377.88235</span><span class="p">,</span><span class="m">389.52229</span><span class="p">,</span><span class="m">456.80425</span><span class="p">,</span><span class="m">476.63287</span><span class="p">,</span><span class="m">500.62093</span><span class="p">,</span><span class="m">387.40349</span><span class="p">,</span><span class="m">411.3118</span><span class="p">,</span><span class="m">345.80676</span><span class="p">,</span><span class="m">495.10716</span><span class="p">,</span><span class="m">480.06531</span><span class="p">,</span><span class="m">376.70185</span><span class="p">,</span><span class="m">348.16283</span><span class="p">,</span><span class="m">414.6251</span><span class="p">,</span><span class="m">366.71778</span><span class="p">,</span><span class="m">431.63915</span><span class="p">,</span><span class="m">436.35943</span><span class="p">,</span><span class="m">383.16603</span><span class="p">,</span><span class="m">542.47853</span><span class="p">,</span><span class="m">367.1504</span><span class="p">,</span><span class="m">440.34278</span><span class="p">,</span><span class="m">450.99055</span><span class="p">,</span><span class="m">455.30676</span><span class="p">,</span><span class="m">487.52261</span><span class="p">,</span><span class="m">504.32483</span><span class="p">,</span><span class="m">396.81118</span><span class="p">,</span><span class="m">454.84742</span><span class="p">,</span><span class="m">430.33206</span><span class="p">,</span><span class="m">501.33838</span><span class="p">,</span><span class="m">335.69292</span><span class="p">,</span><span class="m">461.27979</span><span class="p">),</span>
<span class="w">  </span><span class="n">scie</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">376.45612</span><span class="p">,</span><span class="m">434.89017</span><span class="p">,</span><span class="m">416.65516</span><span class="p">,</span><span class="m">507.91081</span><span class="p">,</span><span class="m">494.86606</span><span class="p">,</span><span class="m">494.80982</span><span class="p">,</span><span class="m">421.72777</span><span class="p">,</span><span class="m">405.9808</span><span class="p">,</span><span class="m">444.34865</span><span class="p">,</span><span class="m">500.98817</span><span class="p">,</span><span class="m">500.80958</span><span class="p">,</span><span class="m">461.86337</span><span class="p">,</span><span class="m">420.46238</span><span class="p">,</span><span class="m">410.20256</span><span class="p">,</span><span class="m">510.04275</span><span class="p">,</span><span class="m">495.53081</span><span class="p">,</span><span class="m">481.59372</span><span class="p">,</span><span class="m">361.725</span><span class="p">,</span><span class="m">491.143</span><span class="p">,</span><span class="m">528.51191</span><span class="p">,</span><span class="m">498.51606</span><span class="p">,</span><span class="m">481.40229</span><span class="p">,</span><span class="m">492.73984</span><span class="p">,</span><span class="m">385.03211</span><span class="p">,</span><span class="m">445.14657</span><span class="p">,</span><span class="m">374.28158</span><span class="p">,</span><span class="m">524.42376</span><span class="p">,</span><span class="m">482.90977</span><span class="p">,</span><span class="m">492.44187</span><span class="p">,</span><span class="m">395.31191</span><span class="p">,</span><span class="m">504.3107</span><span class="p">,</span><span class="m">447.24463</span><span class="p">,</span><span class="m">464.2001</span><span class="p">,</span><span class="m">480.90591</span><span class="p">,</span><span class="m">394.72014</span><span class="p">,</span><span class="m">373.75317</span><span class="p">,</span><span class="m">546.25504</span><span class="p">,</span><span class="m">440.96584</span><span class="p">,</span><span class="m">341.14726</span><span class="p">,</span><span class="m">531.3388</span><span class="p">,</span><span class="m">353.75366</span><span class="p">,</span><span class="m">479.50271</span><span class="p">,</span><span class="m">493.00024</span><span class="p">,</span><span class="m">543.14931</span><span class="p">,</span><span class="m">364.28557</span><span class="p">,</span><span class="m">418.00956</span><span class="p">,</span><span class="m">410.76032</span><span class="p">,</span><span class="m">381.67947</span><span class="p">,</span><span class="m">468.78635</span><span class="p">,</span><span class="m">404.08089</span><span class="p">,</span><span class="m">411.62124</span><span class="p">,</span><span class="m">417.15835</span><span class="p">,</span><span class="m">486.63188</span><span class="p">,</span><span class="m">478.18198</span><span class="p">,</span><span class="m">503.74646</span><span class="p">,</span><span class="m">383.48177</span><span class="p">,</span><span class="m">410.52702</span><span class="p">,</span><span class="m">355.28466</span><span class="p">,</span><span class="m">504.86972</span><span class="p">,</span><span class="m">487.69184</span><span class="p">,</span><span class="m">371.42047</span><span class="p">,</span><span class="m">367.63747</span><span class="p">,</span><span class="m">428.26692</span><span class="p">,</span><span class="m">381.6494</span><span class="p">,</span><span class="m">453.79679</span><span class="p">,</span><span class="m">436.78672</span><span class="p">,</span><span class="m">390.96302</span><span class="p">,</span><span class="m">560.97909</span><span class="p">,</span><span class="m">374.21203</span><span class="p">,</span><span class="m">447.30452</span><span class="p">,</span><span class="m">467.28861</span><span class="p">,</span><span class="m">486.88349</span><span class="p">,</span><span class="m">494.4219</span><span class="p">,</span><span class="m">526.40753</span><span class="p">,</span><span class="m">429.0165</span><span class="p">,</span><span class="m">475.16645</span><span class="p">,</span><span class="m">435.88107</span><span class="p">,</span><span class="m">497.41058</span><span class="p">,</span><span class="m">355.05556</span><span class="p">,</span><span class="m">472.23249</span><span class="p">),</span>
<span class="w">  </span><span class="n">econ_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">-.74391004</span><span class="p">,</span><span class="n">.</span><span class="m">32525118</span><span class="p">,</span><span class="m">-.67148081</span><span class="p">,</span><span class="n">.</span><span class="m">39335586</span><span class="p">,</span><span class="n">.</span><span class="m">09302247</span><span class="p">,</span><span class="n">.</span><span class="m">1154232</span><span class="p">,</span><span class="m">-.27047491</span><span class="p">,</span><span class="m">-.97523877</span><span class="p">,</span><span class="m">-.27116078</span><span class="p">,</span><span class="n">.</span><span class="m">3339245</span><span class="p">,</span><span class="n">.</span><span class="m">1809795</span><span class="p">,</span><span class="m">-.24284266</span><span class="p">,</span><span class="m">-.96515588</span><span class="p">,</span><span class="kc">NA</span><span class="p">,</span><span class="m">-.01307006</span><span class="p">,</span><span class="m">-.12423293</span><span class="p">,</span><span class="n">.</span><span class="m">34054443</span><span class="p">,</span><span class="m">-.71082132</span><span class="p">,</span><span class="n">.</span><span class="m">02354851</span><span class="p">,</span><span class="n">.</span><span class="m">18467558</span><span class="p">,</span><span class="n">.</span><span class="m">19358048</span><span class="p">,</span><span class="m">-.05861012</span><span class="p">,</span><span class="n">.</span><span class="m">11972009</span><span class="p">,</span><span class="m">-.44695238</span><span class="p">,</span><span class="m">-.13303286</span><span class="p">,</span><span class="m">-1.4975343</span><span class="p">,</span><span class="m">-.45290783</span><span class="p">,</span><span class="m">-.1523256</span><span class="p">,</span><span class="n">.</span><span class="m">06026681</span><span class="p">,</span><span class="m">-1.4481847</span><span class="p">,</span><span class="n">.</span><span class="m">34413547</span><span class="p">,</span><span class="n">.</span><span class="m">38376268</span><span class="p">,</span><span class="n">.</span><span class="m">25381603</span><span class="p">,</span><span class="m">-.09725531</span><span class="p">,</span><span class="m">-.57839371</span><span class="p">,</span><span class="m">-.80391485</span><span class="p">,</span><span class="m">-.02354713</span><span class="p">,</span><span class="m">-.26977891</span><span class="p">,</span><span class="m">-2.0669077</span><span class="p">,</span><span class="n">.</span><span class="m">23721704</span><span class="p">,</span><span class="m">-.3564011</span><span class="p">,</span><span class="n">.</span><span class="m">03484754</span><span class="p">,</span><span class="m">-.00981672</span><span class="p">,</span><span class="m">-.44904569</span><span class="p">,</span><span class="m">-1.8117468</span><span class="p">,</span><span class="m">-.50431186</span><span class="p">,</span><span class="m">-.92555928</span><span class="p">,</span><span class="m">-.26902116</span><span class="p">,</span><span class="n">.</span><span class="m">04917327</span><span class="p">,</span><span class="m">-.1922437</span><span class="p">,</span><span class="m">-.73720841</span><span class="p">,</span><span class="m">-.66233746</span><span class="p">,</span><span class="n">.</span><span class="m">23876144</span><span class="p">,</span><span class="n">.</span><span class="m">52544167</span><span class="p">,</span><span class="n">.</span><span class="m">22389294</span><span class="p">,</span><span class="m">-.96079427</span><span class="p">,</span><span class="m">-1.1207972</span><span class="p">,</span><span class="m">-1.3333803</span><span class="p">,</span><span class="m">-.06879525</span><span class="p">,</span><span class="m">-.2048969</span><span class="p">,</span><span class="m">-1.2130009</span><span class="p">,</span><span class="m">-.91617362</span><span class="p">,</span><span class="n">.</span><span class="m">11453604</span><span class="p">,</span><span class="m">-.46005621</span><span class="p">,</span><span class="m">-.29700884</span><span class="p">,</span><span class="m">-.28406969</span><span class="p">,</span><span class="m">-.27803347</span><span class="p">,</span><span class="n">.</span><span class="m">29037254</span><span class="p">,</span><span class="m">-1.3106626</span><span class="p">,</span><span class="m">-.20093189</span><span class="p">,</span><span class="m">-.25804407</span><span class="p">,</span><span class="n">.</span><span class="m">15198116</span><span class="p">,</span><span class="n">.</span><span class="m">33239471</span><span class="p">,</span><span class="m">-.28002015</span><span class="p">,</span><span class="m">-.96258429</span><span class="p">,</span><span class="m">-1.2274957</span><span class="p">,</span><span class="m">-.81679644</span><span class="p">,</span><span class="n">.</span><span class="m">04198107</span><span class="p">,</span><span class="m">-.68010732</span><span class="p">,</span><span class="m">-1.3159208</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">datos</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">na.omit</span><span class="p">(</span><span class="n">datos</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">datos</span><span class="o">$</span><span class="n">econ_index</span><span class="p">,</span><span class="w"> </span><span class="n">datos</span><span class="o">$</span><span class="n">read</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Economic Index&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Reading Test Scores - PISA 2022&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/7ae7e9726d9f2bdceaf9dacefbfc1373fb9d7b838e5ab9b3e550a95fd41306f0.png"><img alt="_images/7ae7e9726d9f2bdceaf9dacefbfc1373fb9d7b838e5ab9b3e550a95fd41306f0.png" src="_images/7ae7e9726d9f2bdceaf9dacefbfc1373fb9d7b838e5ab9b3e550a95fd41306f0.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">read</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">econ_index</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">datos</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = read ~ econ_index, data = datos)

Residuals:
    Min      1Q  Median      3Q     Max 
-98.150 -20.616   3.319  24.557  91.063 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  460.878      5.218  88.331  &lt; 2e-16 ***
econ_index    68.896      7.821   8.809  2.8e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 39.54 on 77 degrees of freedom
Multiple R-squared:  0.5019,	Adjusted R-squared:  0.4955 
F-statistic:  77.6 on 1 and 77 DF,  p-value: 2.797e-13
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Prediccion</span>
<span class="n">model_fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.data.frame</span><span class="p">(</span><span class="nf">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">se.fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;confidence&quot;</span><span class="p">,</span><span class="w">  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">datos</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.99</span><span class="p">))</span>
<span class="nf">names</span><span class="p">(</span><span class="n">model_fit</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&#39;yhat&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;lwr&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;upr&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;se&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;df&#39;</span><span class="p">,</span><span class="w"> </span><span class="s">&#39;residuals&#39;</span><span class="p">)</span>
<span class="nf">head</span><span class="p">(</span><span class="n">model_fit</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A data.frame: 5 × 6</caption>
<thead>
	<tr><th></th><th scope=col>yhat</th><th scope=col>lwr</th><th scope=col>upr</th><th scope=col>se</th><th scope=col>df</th><th scope=col>residuals</th></tr>
	<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>409.6258</td><td>395.3168</td><td>423.9347</td><td>5.417594</td><td>77</td><td>39.54123</td></tr>
	<tr><th scope=row>2</th><td>483.2867</td><td>465.0712</td><td>501.5022</td><td>6.896686</td><td>77</td><td>39.54123</td></tr>
	<tr><th scope=row>3</th><td>414.6158</td><td>401.1048</td><td>428.1269</td><td>5.115512</td><td>77</td><td>39.54123</td></tr>
	<tr><th scope=row>4</th><td>487.9789</td><td>468.6670</td><td>507.2907</td><td>7.311772</td><td>77</td><td>39.54123</td></tr>
	<tr><th scope=row>5</th><td>467.2871</td><td>452.4118</td><td>482.1624</td><td>5.632037</td><td>77</td><td>39.54123</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">plot</span><span class="p">(</span><span class="n">datos</span><span class="o">$</span><span class="n">econ_index</span><span class="p">,</span><span class="w"> </span><span class="n">datos</span><span class="o">$</span><span class="n">read</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Economic Index&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Reading Test Scores - PISA 2022&quot;</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="n">datos</span><span class="o">$</span><span class="n">econ_index</span><span class="p">,</span><span class="w"> </span><span class="n">model_fit</span><span class="o">$</span><span class="n">yhat</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/5cccca952268248c763c1489dd74e5c47a0c6fed27ce311138f773101becb1b2.png"><img alt="_images/5cccca952268248c763c1489dd74e5c47a0c6fed27ce311138f773101becb1b2.png" src="_images/5cccca952268248c763c1489dd74e5c47a0c6fed27ce311138f773101becb1b2.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
</section>
<section id="residuales-de-la-regresion-e-i-y-i-hat-y-i">
<h2>2.6 Residuales de la Regresión: <span class="math notranslate nohighlight">\(e_i = Y_i - \hat Y_i\)</span><a class="headerlink" href="#residuales-de-la-regresion-e-i-y-i-hat-y-i" title="Link to this heading">#</a></h2>
<p>Observe que a partir de las ecuaciones normales tenemos que</p>
<div class="math notranslate nohighlight">
\[\sum_i ({Y_i} - \hat\beta_0 - \hat\beta_1 X_i) = \sum_i e_i = 0\]</div>
<div class="math notranslate nohighlight">
\[\sum_i (X_i) \cdot ({Y_i} - \hat\beta_0 - \hat\beta_1 X_i) = \sum_i e_i \cdot X_i = 0\]</div>
<p>Lo anterior indica que:</p>
<div class="math notranslate nohighlight">
\[\frac{\sum_i e_i}{n} = \bar e = 0 \hspace{5pt} \Rightarrow \hspace{5pt} \bar Y = \bar{\hat Y}\]</div>
<div class="math notranslate nohighlight">
\[\sum_i e_i \cdot X_i = 0 \hspace{5pt} \Rightarrow \hspace{5pt} \mathbb{\hat Cov}(e_i, X_i) = \sum_i (e_i - \bar e) \cdot (X_i - \bar X) = 0\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Predecir los errores</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">datos</span><span class="o">$</span><span class="n">read</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">model_fit</span><span class="o">$</span><span class="n">yhat</span>

<span class="c1">#Estimacion de la función de densidad</span>
<span class="nf">hist</span><span class="p">(</span><span class="n">e</span><span class="p">,</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">T</span><span class="p">)</span>
<span class="nf">lines</span><span class="p">(</span><span class="nf">density</span><span class="p">(</span><span class="n">e</span><span class="p">),</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">lw</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="_images/45e5aa4d613b994598e16c7c5d3e407b91d3a9d7bf92456c319951765124cefc.png"><img alt="_images/45e5aa4d613b994598e16c7c5d3e407b91d3a9d7bf92456c319951765124cefc.png" src="_images/45e5aa4d613b994598e16c7c5d3e407b91d3a9d7bf92456c319951765124cefc.png" style="width: 420px; height: 420px;" />
</a>
</div>
</div>
</section>
<section id="propiedades-de-los-estimadores-hat-beta-k">
<h2>2.7 Propiedades de los estimadores <span class="math notranslate nohighlight">\(\hat\beta_k\)</span><a class="headerlink" href="#propiedades-de-los-estimadores-hat-beta-k" title="Link to this heading">#</a></h2>
<p>Queremos evaluar si <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> es un buen estimador de <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>La primera pregunta que nos hacemos es ¿<span class="math notranslate nohighlight">\(E(\hat\beta_1) = \beta_1\)</span>? Es decir, queremos ver si <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> es un estimador insesgado.</p>
<p>Note que</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \hat\beta_1 &amp; = \frac{\sum_i (X_i - \overline X)(Y_i - \overline Y)}{\sum_i (X_i - \overline X)^2}\\
     &amp; = \frac{\sum_i (X_i - \overline X)(\beta_0 + \beta_1 X_i + \varepsilon_i - \beta_0 - \beta_1 \bar X - \bar \varepsilon)}{\sum_i (X_i - \overline X)^2}\\
    &amp; = \beta_1 + \frac{\sum_i (X_i - \overline X)(\varepsilon_i - \overline \varepsilon)}{\sum_i (X_i - \overline X)^2}\\
    &amp; = \beta_1 + \frac{\sum_i (X_i - \overline X) \cdot \varepsilon_i}{\sum_i (X_i - \overline X)^2} \\     
\end{align*}\]</div>
<p>Si tomamos la esperanza de esta exprensión tenemos:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E(\hat\beta_1) &amp; = \beta_1 + E \left(\frac{\sum_i (X_i - \overline X) \cdot \varepsilon_i}{\sum_i (X_i - \overline X)^2}\right) \\    
\end{align*}\]</div>
<p>A partir del supuesto de <strong>independencia condicional</strong>, para todo <span class="math notranslate nohighlight">\(i\)</span> y <span class="math notranslate nohighlight">\(j\)</span>, <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> y <span class="math notranslate nohighlight">\(X_j\)</span> son independientes. Es decir,</p>
<div class="math notranslate nohighlight">
\[E(\varepsilon_i \cdot X_j) = E(\varepsilon_i) \cdot E(X_j) = 0\]</div>
<p>Debido a esto:</p>
<div class="math notranslate nohighlight">
\[E\left(\sum_i (X_i - \overline X) \cdot \varepsilon_i\right) = \sum_i E(X_i \varepsilon_i) - \sum_i E(\overline X \varepsilon_i) = \sum_i E(X_i) E(\varepsilon_i) - \sum_i E(\overline X) E(\varepsilon_i) = 0\]</div>
<p>De esta manera, el estimador <span class="math notranslate nohighlight">\(\hat\beta_1\)</span>, que obtuvimos por el método MCO, es insesgado. Así,</p>
<div class="math notranslate nohighlight">
\[E(\hat\beta_1) = \beta_1\]</div>
<p>Es <span class="math notranslate nohighlight">\(\beta_0\)</span> un estimador insesgado?</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E(\hat\beta_0) &amp; =E \left(\overline Y - \hat\beta_1 \overline X \right)\\
     &amp; = E\left(\beta_0 + \beta_1 \overline X + \overline\varepsilon - \hat\beta_1 \overline X\right)\\
     &amp; = \beta_0 + \beta_1 E(\overline X) + E(\overline\varepsilon) - \beta_1 E(\overline X)\\
     &amp; = \beta_0 \\
\end{align*}\]</div>
</section>
<section id="inferencia-en-regresion-simple">
<h2>2.8 Inferencia en Regresión Simple<a class="headerlink" href="#inferencia-en-regresion-simple" title="Link to this heading">#</a></h2>
<p>Luego de estimar el modelo, y saber que en promedio nuestros estimadores de MCO encuentran los verdaderos parámetros que se desconocen, queremos determinar si</p>
<ol class="arabic simple">
<li><p>existe una relación <strong>estádisticamente significativa</strong> entre <span class="math notranslate nohighlight">\(Y\)</span> y <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>nuestro modelo tiene un buen ajuste (o poder predictivo)</p></li>
</ol>
<p>Con este objetivo en mente,</p>
<ol class="arabic simple">
<li><p>realizaremos pruebas de hipotesis individuales y conjuntas</p></li>
<li><p>computaremos el coeficiente de determinación <span class="math notranslate nohighlight">\(R^2\)</span>.</p></li>
</ol>
<section id="pruebas-de-hipotesis-sobre-hat-beta-k">
<h3>2.8.1 Pruebas de Hipótesis sobre <span class="math notranslate nohighlight">\(\hat\beta_k\)</span><a class="headerlink" href="#pruebas-de-hipotesis-sobre-hat-beta-k" title="Link to this heading">#</a></h3>
<p>Debido al supuesto de <strong>normalidad de los errores</strong>, <span class="math notranslate nohighlight">\(\varepsilon_i | X \sim N(0, \sigma^2)\)</span>, es posible probar que</p>
<div class="math notranslate nohighlight">
\[\hat\beta_k | X \sim N(\beta_k, \sigma^2_\beta)\]</div>
<p>Sin entrar en detalles, recuerde que la suma de variables normales da como resultado una variable que se distribuye normal. Esta es la idea fundamental detrás de la demostración de que nuestros estimadores de MCO son normales.</p>
<p>La hipotesis que queremos considerar en general toma la siguiente forma:</p>
<div class="math notranslate nohighlight">
\[H_0 : \beta_k = 0\]</div>
<div class="math notranslate nohighlight">
\[H_1 : \beta_k \neq 0\]</div>
<p>Note que bajo la hipótesis nula (<span class="math notranslate nohighlight">\(H_0\)</span>) el parámetro <span class="math notranslate nohighlight">\(\beta_k\)</span> es cero, y queremos saber si podemos rechazar dicha hipotesis con un nivel de significancia <span class="math notranslate nohighlight">\(\alpha \in \{0.1, 0.05, 0.01\}\)</span>.</p>
<p>Para probar esta hipótesis podemoslos usar el siguiente estadístico:</p>
<div class="math notranslate nohighlight">
\[\frac{\hat\beta_k - \beta_k}{\sigma_\beta}\]</div>
<p>Sin embargo, desconocemos <span class="math notranslate nohighlight">\(\sigma_\beta\)</span>. Esto implica que debemos estimarlo.</p>
</section>
<section id="varianza-de-hat-beta-k">
<h3>2.8.2 Varianza de <span class="math notranslate nohighlight">\(\hat\beta_k\)</span><a class="headerlink" href="#varianza-de-hat-beta-k" title="Link to this heading">#</a></h3>
<p>Recuerde <span class="math notranslate nohighlight">\(\hat\beta_1 = \frac{\mathbb{\hat Cov}(X, Y)}{\mathbb{\hat V}(X)} = \beta_1 + \frac{\sum_i (X_i - \overline X) \cdot \varepsilon_i}{\sum_i (X_i - \overline X)^2}\)</span>. De esta manera,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    V(\hat\beta_1 | X) &amp; = E \left(\hat\beta_1 - E(\hat\beta_1) | X \right)^2 = E \left(\hat\beta_1 - \beta_1 | X \right)^2 = E \left(\frac{\sum_i (X_i - \overline X) \cdot \varepsilon_i}{\sum_i (X_i - \overline X)^2} | X \right)^2 \\ 
    &amp; = \left(\frac{1}{\sum_i (X_i - \overline X)^2}\right)^2 E \left(\sum_i (X_i - \overline X) \cdot \varepsilon_i | X \right)^2 \\
    &amp; = \left(\frac{1}{\sum_i (X_i - \overline X)^2}\right)^2 E \left(\sum_i (X_i - \overline X)^2 \cdot \varepsilon_i^2 + 2 \sum_{i &lt; j} (X_i - \overline X) (X_j - \overline X) \varepsilon_i \varepsilon_j | X \right) \\
    &amp; = \left(\frac{1}{\sum_i (X_i - \overline X)^2}\right)^2 \left(\sum_i (X_i - \overline X)^2 \cdot E(\varepsilon_i^2 | X) \right) \\ 
    &amp; = \frac{\sigma^2}{\sum_i (X_i - \overline X)^2} = \frac{\sigma^2}{S^2_X (n-1)} \\
\end{align*}\]</div>
<p>Note que para estimar esta varianza debemos encontrar un estimador para <span class="math notranslate nohighlight">\(\sigma^2\)</span></p>
</section>
<section id="varianza-de-los-errores">
<h3>2.8.3 Varianza de los Errores<a class="headerlink" href="#varianza-de-los-errores" title="Link to this heading">#</a></h3>
<p>Antes de encontrar un estimador para <span class="math notranslate nohighlight">\(\sigma^2\)</span>, note que</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    V(Y_i | X_i) &amp; = V(\beta_0 + \beta_1 X_i + \varepsilon_i | X) \\
    &amp; = V(\beta_0 + \beta_1 X_i | X) + V(\varepsilon_i | X) + 2 \cdot Cov(\beta_0 + \beta_1 X_i, \varepsilon_i | X) \\
    &amp; = V(\varepsilon_i | X) = \sigma^2 \\
\end{align*}\]</div>
<p>Más aún, observe que</p>
<div class="math notranslate nohighlight">
\[Y_i | X \sim N(\beta_0 + \beta_1 X_i, \sigma^2)\]</div>
<p>Ahora, ya que <span class="math notranslate nohighlight">\(E(\varepsilon_i) = 0\)</span>, podemos mostrar que</p>
<div class="math notranslate nohighlight">
\[V(\varepsilon_i | X) = E(\varepsilon_i^2)\]</div>
<p>Un estimador para <span class="math notranslate nohighlight">\(\sigma^2\)</span> es:</p>
<div class="math notranslate nohighlight">
\[\hat\sigma^2 = \frac{\sum_i e_i^2}{n-K} = \frac{\sum_i (Y_i - \hat Y_i)^2}{n-K} = \frac{\sum_i (Y_i - \hat\beta_0 - \hat\beta_1 X_i)^2}{n-K} \]</div>
<p>Donde <span class="math notranslate nohighlight">\(K\)</span> es el número de parámetros que debemos estimar. En este caso <span class="math notranslate nohighlight">\(K = 2\)</span>.</p>
</section>
<section id="pruebas-de-hipotesis-individuales">
<h3>2.8.4 Pruebas de Hipótesis Individuales<a class="headerlink" href="#pruebas-de-hipotesis-individuales" title="Link to this heading">#</a></h3>
<p>Para nuestra prueba de hipótesis</p>
<div class="math notranslate nohighlight">
\[H_0 : \beta_1 = 0\]</div>
<div class="math notranslate nohighlight">
\[H_1 : \beta_1 \neq 0\]</div>
<p>Usaremos el siguiente estadístico de prueba</p>
<div class="math notranslate nohighlight">
\[T = \frac{\hat\beta_1 - \beta_1}{\text{ee}(\hat\beta_1)}\]</div>
<p>Donde el error estandar de <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> se define como:</p>
<div class="math notranslate nohighlight">
\[\text{ee}(\hat\beta_1) = \sqrt{ \frac{\sum_i e_i^2/n-K}{\sum_i (X_i - \overline X)^2}}\]</div>
<p>Observe que nuestro estadístico <span class="math notranslate nohighlight">\(T \sim t_{n-K}\)</span>. Esto se debe a que <span class="math notranslate nohighlight">\(\hat\beta_1 \sim N(\beta_1, \sigma_{\beta_1})\)</span> y <span class="math notranslate nohighlight">\(\sum_i e_i^2 \sim \chi^2_{n-K}\)</span></p>
</section>
<section id="intervalo-de-confianza-para-beta-k">
<h3>2.8.4 Intervalo de confianza para <span class="math notranslate nohighlight">\(\beta_k\)</span><a class="headerlink" href="#intervalo-de-confianza-para-beta-k" title="Link to this heading">#</a></h3>
<p>Nuestro intervalo de confianza para el parámetro desconocido <span class="math notranslate nohighlight">\(\beta_k\)</span> está dado por</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    IC &amp; = \left(\beta_k - t_{\alpha/2 \hspace{2pt},\hspace{2pt} n-K}\cdot\sqrt{ \frac{\sum_i e_i^2/n-K}{\sum_i (X_i - \overline X)^2}} \hspace{5pt} , \hspace{5pt} \beta_k + t_{\alpha/2 \hspace{2pt},\hspace{2pt} n - K}\cdot\sqrt{ \frac{\sum_i e_i^2/n-K}{\sum_i (X_i - \overline X)^2}} \right)
\end{align*}\]</div>
<p>Cuando <span class="math notranslate nohighlight">\(n\)</span> es grande, si <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, entonces <span class="math notranslate nohighlight">\(|t_{\alpha/2 \hspace{2pt},\hspace{2pt} n-K}| \approx 1.96\)</span></p>
</section>
<section id="coeficiente-de-determinacion-r-2">
<h3>2.8.5 Coeficiente de Determinación: <span class="math notranslate nohighlight">\(R^2\)</span><a class="headerlink" href="#coeficiente-de-determinacion-r-2" title="Link to this heading">#</a></h3>
<p>Una segunda forma de evaluar nuestro modelo es sabiendo qué tanto de <span class="math notranslate nohighlight">\(\boldsymbol{Y}\)</span> puede <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> explicar.</p>
<p>Para saber esto debemos hacer lo siguiente:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Y_i  &amp;= \hat Y_i + e_i \\
Y_i -\bar{Y} &amp;= \hat Y_i -\bar{Y} + e_i \\
\sum_i (Y_i -\bar{Y})^2 &amp;= \sum_i (\hat Y_i -\bar{Y} + e_i)^2 \\
\sum_i (Y_i -\bar{Y})^2 &amp;= \sum_i (\hat Y_i -\bar{Y})^2 + \sum_i e_i^2 + 2 \sum_i e_i (\hat Y_i -\bar{Y}) \\
\sum_i (Y_i -\bar{Y})^2 &amp;= \sum_i (\hat Y_i -\bar{Y})^2 + \sum_i e_i^2
\end{align*}\]</div>
<p>Esto es así porque</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_i e_i (\hat Y_i -\bar{Y}) &amp;= \sum_i e_i \hat Y_i - \bar{Y}\sum e_i \\
&amp;= \sum_i e_i(\hat \beta_0 + \hat \beta_1 X_i) - \bar{Y}\sum e_i = 0
\end{align*}\]</div>
<p>Es decir, a partir de nuestros estimadores de MCO</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\underbrace{\sum_i (Y_i -\bar{Y})^2}_{\text{Varianza Total } (SST)} &amp;= \sum_i (\hat Y_i -\bar{Y})^2 + \sum_i e_i^2 \\
&amp;= \underbrace{\sum_i (\hat Y_i -\bar{Y})^2}_{\text{Varianza Explicada } (SSR)} + \underbrace{\sum_i (Y_i - \hat Y_i)^2}_{\text{Varianza No Explicada } (SSE)}
\end{align*}\]</div>
<p>Luego podemos definir:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
R^2 &amp;= \frac{SSR}{SST} = 1 - \frac{SSE}{SST} \\
&amp;= \frac{\sum_i (\hat Y_i -\bar{Y})^2}{\sum_i (Y_i -\bar{Y})^2} = 1 - \frac{\sum_i (Y_i - \hat Y_i)^2}{\sum_i (Y_i -\bar{Y})^2}
\end{align*}\]</div>
<p>Observe que el coeficiente de determinación está acodato, es decir,</p>
<div class="math notranslate nohighlight">
\[0 \leq R^2 \leq 1\]</div>
<p>Este estadístico <strong>mide qué tanto de la varianza de <span class="math notranslate nohighlight">\(Y\)</span> es explicada por la covariable <span class="math notranslate nohighlight">\(X\)</span></strong></p>
<div class="alert alert-block alert-danger"> 
<b>Nota:</b>
<p>
<p>En el modelo de regresión lineal, el coeficiente de determinación crece a medida que se incluyen más variables explicativas o covariables.</p>
</div>
<p>Para el modelo de regresión simple podemos demostar que</p>
<div class="math notranslate nohighlight">
\[\rho_{Y,X}^2 = R^2\]</div>
<p>donde <span class="math notranslate nohighlight">\(\rho_{Y,X}\)</span> es el coeficiente de correlación entre <span class="math notranslate nohighlight">\(X\)</span> y <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</section>
<section id="prueba-de-significancia-global">
<h3>2.8.6 Prueba de Significancia Global<a class="headerlink" href="#prueba-de-significancia-global" title="Link to this heading">#</a></h3>
<p>La idea de esta prueba es evaluar si todas las covariables que incluimos en el modelo pueden explicar conjuntamente la variable <span class="math notranslate nohighlight">\(Y\)</span></p>
<p>La prueba de signifiancia global se define como:</p>
<div class="math notranslate nohighlight">
\[H_0 : \beta_1 = \beta_2 = ... = \beta_K = 0\]</div>
<div class="math notranslate nohighlight">
\[H_1 : \text{ al menos un } \beta_k \text{ es diferente de 0}\]</div>
<p>Esta es una prueba de ‘cola derecha,’ por lo tanto la zona de rechazo siempre va a ser a la derecha e igual a <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Observe que en el modelo de regresión univariada (o simple), esta prueba es equivalente a:</p>
<div class="math notranslate nohighlight">
\[H_0 : \beta_1 = 0\]</div>
<div class="math notranslate nohighlight">
\[H_1 : \beta_1 \neq 0\]</div>
<p>Para evaluar esta prueba de hipótesis debemos plantear un nuevo estadístico.</p>
<p>Antes de plantear el estadístico de prueba debemos construir los <strong>cuadrados medios</strong>.</p>
<p>Los cuadrados medios se definen <strong>como la correspondiente suma de cuadrados dividido por sus grados de libertad</strong>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Suma de Cuadrados</p></th>
<th class="head"><p>G.L.</p></th>
<th class="head"><p>Cuadrados Medios</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Modelo</strong></p></td>
<td><p>$<span class="math notranslate nohighlight">\(\sum(\hat{Y}_i - \bar{Y})^2\)</span>$</p></td>
<td><p>$<span class="math notranslate nohighlight">\(K-1\)</span>$</p></td>
<td><p>$<span class="math notranslate nohighlight">\(\dfrac{\sum(\hat{Y}_i - \bar{Y})^2}{K-1}\)</span>$</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Residuales</strong></p></td>
<td><p>$<span class="math notranslate nohighlight">\(\sum(Y_i - \hat Y_i)^2\)</span>$</p></td>
<td><p>$<span class="math notranslate nohighlight">\(N-K\)</span>$</p></td>
<td><p>$<span class="math notranslate nohighlight">\(\dfrac{\sum(Y_i - \hat Y_i)^2}{N-K}\)</span>$</p></td>
</tr>
<tr class="row-even"><td><p><strong>Total</strong></p></td>
<td><p>$<span class="math notranslate nohighlight">\(\sum(Y_i - \bar{Y})^2\)</span>$</p></td>
<td><p>$<span class="math notranslate nohighlight">\(N-1\)</span>$</p></td>
<td><p>$<span class="math notranslate nohighlight">\(\dfrac{\sum(Y_i - \bar{Y})^2}{N-1}\)</span>$</p></td>
</tr>
</tbody>
</table>
</div>
<p>Vamos a llamar los cuadrados medios como:</p>
<div class="math notranslate nohighlight">
\[MSR = SSR/K-1\]</div>
<div class="math notranslate nohighlight">
\[MSE = SSE/N-K\]</div>
<div class="math notranslate nohighlight">
\[MST = SST/N-1\]</div>
<p>Con base en estos estadísticos podemos calcular nuestro estadístico de prueba:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
F_{K-1,N-K} &amp;= \frac{MSR}{MSE} \\
&amp;= \frac{\frac{\sum(\hat Y_i - \bar{Y})^2}{K-1}}{\frac{\sum(Y_i - \hat Y_i)^2}{N-K}}
\end{align*}\]</div>
<p>Observe que</p>
<div class="math notranslate nohighlight">
\[MSE = \frac{\sum(Y_i - \hat Y_i)^2}{N-K} = \hat{\sigma}^2\]</div>
<p><span class="math notranslate nohighlight">\(MSE\)</span> es lo mismo que el estimador de la varianza de los errores. Entonces el <strong>error estándar del modelo</strong> es igual a:</p>
<div class="math notranslate nohighlight">
\[\hat{\sigma} = \sqrt{MSE} = \sqrt{\frac{\sum(y_i - \hat{y}_i)^2}{N-K}}\]</div>
<p>También en regresión simple:</p>
<div class="math notranslate nohighlight">
\[T^2 = F\]</div>
</section>
<section id="prediccion-e-inferencia-para-hat-y-i">
<h3>2.8.7 Predicción e Inferencia para <span class="math notranslate nohighlight">\(\hat Y_i\)</span><a class="headerlink" href="#prediccion-e-inferencia-para-hat-y-i" title="Link to this heading">#</a></h3>
<p>Después de evaluar nuestro modelo podemos usarlo y hacer predicciones:</p>
<div class="math notranslate nohighlight">
\[\hat Y_i = \hat\beta_0 + \hat\beta_1 X_i\]</div>
<p>Es decir, a partir de las observaciones <span class="math notranslate nohighlight">\(X_i\)</span> podemos intentar predecir el valor <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
<p>Observe que <span class="math notranslate nohighlight">\(\hat Y_i\)</span> es una variable aleatoria y debemos hacer inferencia sobre este resultado.</p>
<p>Se pueden computar dos tipos de intervalos de confianza:</p>
<ol class="arabic simple">
<li><p>Un intervalo sobre la predicción media o <span class="math notranslate nohighlight">\(E(Y|X)\)</span></p></li>
<li><p>Un intervalo para hacer predicción sobre un valor individual <span class="math notranslate nohighlight">\(\hat Y_i\)</span></p></li>
</ol>
</section>
<section id="prediccion-media">
<h3>2.8.8 Predicción Media<a class="headerlink" href="#prediccion-media" title="Link to this heading">#</a></h3>
<p>Cuando hacemos predicción sobre la media estamos interesados en sacar un intervalo para el <span class="math notranslate nohighlight">\(E[Y_0|X_0] = \beta_0 + \beta_1 X_0 \)</span>.</p>
<p>Para esto recordemos primero que <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> es un estimador de <span class="math notranslate nohighlight">\(E[Y|X]\)</span>.</p>
<p>Necesitamos entonces la distribución de <span class="math notranslate nohighlight">\(\hat{y}_i\)</span>. Podemos demostrar que:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_0 \sim N(\beta_0 + \beta_1 x_0,Var(\hat{y}_0))\]</div>
<p>Donde</p>
<div class="math notranslate nohighlight">
\[Var(\hat{y}_0) = \sigma^2 \left[ \frac{1}{n} + \frac{ (x_0 - \bar{x})^2}{\sum x_i^2}\right]\]</div>
<p>Entonces reemplazando con <span class="math notranslate nohighlight">\(s^2\)</span>, podemos construir un estimador <span class="math notranslate nohighlight">\(t\)</span> tal que:</p>
<div class="math notranslate nohighlight">
\[t_{n-K} = \frac{\hat{y}_0 - (\beta_0 + \beta_1 x_0)}{ee(\hat{y}_0)}\]</div>
<p>Podemos también sacar un IC:</p>
<div class="math notranslate nohighlight">
\[\hat{y} \pm t_{\alpha/2} ee(\hat{y}_0)\]</div>
</section>
<section id="prediccion-individual">
<h3>2.8.9 Predicción Individual<a class="headerlink" href="#prediccion-individual" title="Link to this heading">#</a></h3>
<p>Queremos sacar un intervalo para un punto en particular <span class="math notranslate nohighlight">\(y_0 = \beta_1 + \beta_2 x_0 + \epsilon_0\)</span></p>
<p><strong>Este intervalo va a ser más grande!</strong></p>
<p>En este caso, <span class="math notranslate nohighlight">\(\hat{y}_0\)</span> será de nuevo nuestro estimador, pero la varianza va a estar dada por la varianza del error de predicción:</p>
<div class="math notranslate nohighlight">
\[Var(y_0 - \hat{y}_0) = \sigma^2 \left[ 1 + \frac{1}{n} + \frac{ (x_0 - \bar{x})^2}{\sum x_i^2}\right]\]</div>
<p>Y el estadístico de prueba estará dado por:</p>
<div class="math notranslate nohighlight">
\[t = \frac{y_0 - \hat{y}_0}{ee(y_0 - \hat{y}_0)}\]</div>
</section>
</section>
<section id="estimacion-por-maxima-verosimilitud">
<h2>2.9 Estimación por Máxima Verosimilitud<a class="headerlink" href="#estimacion-por-maxima-verosimilitud" title="Link to this heading">#</a></h2>
<p>Para estimar los coeficientes <span class="math notranslate nohighlight">\(\beta_k\)</span> también podemos usar el método de máxima verosimilitud.</p>
<p>Recuerde que asumimos que <span class="math notranslate nohighlight">\(\varepsilon_i | X \sim N(0, \sigma^2)\)</span>. Est que implica que</p>
<div class="math notranslate nohighlight">
\[Y_i | X \sim N(\mu_i, \sigma^2)\]</div>
<p>Donde <span class="math notranslate nohighlight">\(\mu_i = \beta_0 + \beta_1 X_i\)</span>. Ya que asumimos que <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> y <span class="math notranslate nohighlight">\(\varepsilon_j\)</span> son independientes para todo <span class="math notranslate nohighlight">\(i\)</span> y <span class="math notranslate nohighlight">\(j\)</span>, la función de verosimilitud está dada por</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\beta_0, \beta_1, \sigma^2) = f(Y_1, Y_2,..., Y_n) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left\{ - \frac{1}{2\sigma^2} (Y_i - \beta_0 - \beta_1 X_i)^2 \right\}\]</div>
<p>Si tomamos el logaritmo de la función de verosimilitud obtenemos:</p>
<div class="math notranslate nohighlight">
\[\mathcal{l}(\beta_0, \beta_1, \sigma^2) =  - \frac{n}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 X_i)^2\]</div>
<p>Maximizando <span class="math notranslate nohighlight">\(\mathcal{l}(\beta_0, \beta_1, \sigma^2)\)</span> respecto a <span class="math notranslate nohighlight">\(\beta_0\)</span> y <span class="math notranslate nohighlight">\(\beta_1\)</span>, encontramos que</p>
<div class="math notranslate nohighlight">
\[\hat\beta_0 = \overline Y - \hat\beta_1 \overline X\]</div>
<div class="math notranslate nohighlight">
\[\hat\beta_1 = \frac{\sum_i (X_i - \overline X)(Y_i - \overline Y)}{\sum_i (X_i - \overline X)^2} = \frac{\mathbb{\hat Cov}(X, Y)}{\mathbb{\hat V}(X)}\]</div>
<p>Observe que estos son precisamente los estimadores que obtuvimos a través de MCO, y ya conocemos las propiedades de estos estimadores.</p>
</section>
<section id="modelo-de-regresion-multiple">
<h2>2.10 Modelo de Regresión Multiple<a class="headerlink" href="#modelo-de-regresion-multiple" title="Link to this heading">#</a></h2>
<p>Un modelo de regresión con múltiples covariables o predictores se puede escribir de la siguiente manera:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \beta_1 x_{1,i} + ... + \beta_{k-1} x_{k-1, i} + \varepsilon_i\]</div>
<p>Observe que queremos estimar <span class="math notranslate nohighlight">\(k\)</span> parámetros (incluyendo a <span class="math notranslate nohighlight">\(\beta_0\)</span>), usando <span class="math notranslate nohighlight">\(k-1\)</span> covariables a partir de una base de datos con <span class="math notranslate nohighlight">\(n\)</span> observaciones</p>
<div class="math notranslate nohighlight">
\[\{y_i, x_{1,i}, x_{2,i}, ..., x_{k-1,i}\}_{i=1}^n\]</div>
<p>Pero antes de derivar los estimadores de los parámetros <span class="math notranslate nohighlight">\(\beta_j\)</span>, veamos una aplicación:</p>
<section id="modelo-en-forma-matricial">
<h3>2.10.1 Modelo en Forma Matricial<a class="headerlink" href="#modelo-en-forma-matricial" title="Link to this heading">#</a></h3>
<p>En forma matricial, nuestro modelo de regresión se puede escribir de la siguiente forma:</p>
<div class="math notranslate nohighlight">
\[Y = X\beta + \varepsilon\]</div>
<p>Observe que tenemos cuatro matrices:</p>
<div class="math notranslate nohighlight">
\[
Y_{[n \times 1]} = X_{[n \times k]} \, \beta_{[k \times 1]} + \varepsilon_{[n \times 1]}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
1 &amp; x_{11} &amp; x_{21} &amp; \dots &amp; x_{k-1,1} \\
1 &amp; x_{12} &amp; x_{22} &amp; \dots &amp; x_{k-1,2} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{1n} &amp; x_{2n} &amp; \dots &amp; x_{k-1,n}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_{k-1}
\end{bmatrix}
+
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}
\end{split}\]</div>
<p>La forma matricial es una manera útil de presentar <span class="math notranslate nohighlight">\(n\)</span> <strong>ecuaciones</strong>, ya que:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
\beta_0 + \sum_j^{k-1} \beta_j x_{j1} + \varepsilon_1 \\
\beta_0 + \sum_j^{k-1} \beta_j x_{j2} + \varepsilon_2 \\
\vdots \\
\beta_0 + \sum_j^{k-1} \beta_j x_{jn} + \varepsilon_n
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="supuestos-en-forma-matricial">
<h3>2.10.2 Supuestos en forma Matricial<a class="headerlink" href="#supuestos-en-forma-matricial" title="Link to this heading">#</a></h3>
<p>¿Cómo se ven los supuestos sobre el error cuando tenemos matrices?</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\varepsilon \varepsilon' =
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}
\begin{bmatrix}
\varepsilon_1 &amp; \varepsilon_2 &amp; \dots &amp; \varepsilon_n
\end{bmatrix}
=
\begin{bmatrix}
\varepsilon_1^2 &amp; \varepsilon_1\varepsilon_2 &amp; \dots &amp; \varepsilon_1\varepsilon_n \\
\varepsilon_2\varepsilon_1 &amp; \varepsilon_2^2 &amp; \dots &amp; \varepsilon_2\varepsilon_n \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\varepsilon_n\varepsilon_1 &amp; \varepsilon_n\varepsilon_2 &amp; \dots &amp; \varepsilon_n^2
\end{bmatrix}
\end{split}\]</div>
<p>Observe que bajo homoscedasticidad y no autocorrelación:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
E[\varepsilon \varepsilon'] =
\begin{bmatrix}
\sigma^2 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{bmatrix}
= \sigma^2 I_n
\end{split}\]</div>
<p>Si asumimos independencia condicional y además que los errores son normales, entonces:</p>
<div class="math notranslate nohighlight">
\[E(\varepsilon | x_1, ...., x_{k-1}) = E(\varepsilon | X) = 0\]</div>
<div class="math notranslate nohighlight">
\[\varepsilon \sim N(0, \sigma^2 I_n)\]</div>
</section>
</section>
<section id="minimos-cuadrados-ordinarios">
<h2>2.11 Mínimos Cuadrados Ordinarios<a class="headerlink" href="#minimos-cuadrados-ordinarios" title="Link to this heading">#</a></h2>
<p>Antes de derivar nuestros estimadores de mínimos cuadrados usando matrices, veremos que es posible encontrar los estimadores de manera similar que en el caso de regresión simple. Nuestro modelo está dado por:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \sum_{j = 1}^{k-1} \beta_j x_{j,i} + \varepsilon_i\]</div>
<p>Así, definimos la función de perdida cómo:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\beta_0, ..., \beta_{k-1}) = \sum_i^n \varepsilon_i^2 = \sum_i^n \left(y_i - \beta_0 - \sum_{j = 1}^{k-1} \beta_j x_{j,i} \right)^2\]</div>
<p>Para encontrar los estimadores de MCO para <span class="math notranslate nohighlight">\(\beta_0, ..., \beta_{k-1}\)</span> debemos resolver el siguiente problema:</p>
<div class="math notranslate nohighlight">
\[\min_{\beta_0, ..., \beta_{k-1}} \mathcal{L}(\beta_0, ..., \beta_{k-1}) = \sum_i^n \varepsilon_i^2\]</div>
<p>Las condiciones de primer orden para este problema están dadas por:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial\mathcal{L}}{\partial\beta_0} \bigg|_{\hat\beta} = \sum_i (-2) \cdot ({y_i} - \beta_0 - \sum_{j = 1}^{k-1} \beta_j x_{j,i}) \bigg|_{\hat\beta} = 0\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial\mathcal{L}}{\partial\beta_1} \bigg|_{\hat\beta} = \sum_i (-2 x_{1,i}) \cdot ({y_i} - \beta_0 \sum_{j = 1}^{k-1} \beta_j x_{j,i}) \bigg|_{\hat\beta} = 0\]</div>
<div class="math notranslate nohighlight">
\[\vdots\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial\mathcal{L}}{\partial\beta_{k-1}} \bigg|_{\hat\beta} = \sum_i (-2 x_{k-1,i}) \cdot ({y_i} - \beta_0 - \sum_{j = 1}^{k-1} \beta_j x_{j,i}) \bigg|_{\hat\beta} = 0\]</div>
<p>Observe que tenemos <span class="math notranslate nohighlight">\(k\)</span> ecuaciones para estimar <span class="math notranslate nohighlight">\(k\)</span> parámetros.</p>
<p>Lo anterior nos permite concluir que este sistema de ecuaciones tiene solución. Sin embargo, solucionarlo alegabraicamente puede ser bastante tedioso. Por esta razón usaremos matrices.</p>
</section>
<section id="estimacion-de-beta">
<h2>2.12 Estimación de <span class="math notranslate nohighlight">\(\beta\)</span><a class="headerlink" href="#estimacion-de-beta" title="Link to this heading">#</a></h2>
<p>Observe que matricialmente podemos escribir</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\varepsilon' \varepsilon =
\begin{bmatrix}
\varepsilon_1 &amp; \varepsilon_2 &amp; \dots &amp; \varepsilon_n
\end{bmatrix}_{1 \times n}
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}_{\hspace{2pt} n \times 1} = 
\varepsilon_1^2 + \varepsilon_2^2 + ... + \varepsilon_n^2 =
\sum_{i = 1}^n \varepsilon_i^2
\end{split}\]</div>
<p>Luego nuestro problema de minimización se puede reescribir de la siguiente manera:</p>
<div class="math notranslate nohighlight">
\[\min_\beta \varepsilon'\varepsilon = \min_\beta (Y - X\beta)'(Y - X\beta) = \min_\beta Y'Y - 2\beta'X'Y + \beta' X'X\beta\]</div>
<p>Nota: <span class="math notranslate nohighlight">\(Y'X\beta = \beta'X'Y\)</span> ya que son escalares.</p>
<p>Las condiciones de primer orden están dadas por</p>
<div class="math notranslate nohighlight">
\[\frac{\partial\mathcal{(\varepsilon'\varepsilon)}}{\partial\beta} \bigg|_{\beta = \hat \beta} = -2 X'Y + 2 X'X\hat\beta = 0\]</div>
<p>Esto es así porque <span class="math notranslate nohighlight">\(\frac{\partial(X'Y)'\beta}{\partial\beta} = X'Y\)</span>, <span class="math notranslate nohighlight">\(\frac{\partial \beta' X'X\beta}{\partial\beta} = (X'X + (X'X)')\beta = 2X'X\beta\)</span></p>
<p>De esta manera, el estimador de MCO para <span class="math notranslate nohighlight">\(\beta\)</span> es</p>
<div class="math notranslate nohighlight">
\[\hat\beta = (X'X)^{-1}(X'Y)\]</div>
<p>Observe que a partir de las condiciones de primer orden podemos determinar que</p>
<div class="math notranslate nohighlight">
\[0 = X'Y - X'X\hat\beta = X'(Y - X'X\hat\beta) = X'e\]</div>
<section id="matrices-y-sumatorias">
<h3>2.12.1 Matrices y Sumatorias<a class="headerlink" href="#matrices-y-sumatorias" title="Link to this heading">#</a></h3>
<p>La matriz <span class="math notranslate nohighlight">\((X'X)^{-1}\)</span> es conocida usalmente como <strong>matriz de información</strong>. Observe que</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X'X =
\begin{bmatrix}
n &amp; \sum_i x_{1i} &amp; \sum_i x_{2i} &amp; \sum_i x_{3i} &amp; \cdots &amp; \sum_i x_{k-1, i} \\
\sum_i x_{1i} &amp; \sum_i x_{1i}^2 &amp; \sum_i x_{1i}x_{2i} &amp; \sum_i x_{1i}x_{3i} &amp; \cdots &amp; \sum_i x_{1i}x_{k-1,i} \\
\sum_i x_{2i} &amp; \sum_i x_{2i}x_{1i} &amp; \sum_i x_{2i}^2 &amp; \sum_i x_{2i}x_{3i} &amp; \cdots &amp; \sum_i x_{2i}x_{k-1,i} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sum_i x_{k-1,i} &amp; \sum_i x_{k-1,i}x_{1i} &amp; \sum_i x_{k-1,i}x_{2i} &amp; \sum_i x_{k-1,i}x_{3i} &amp; \cdots &amp; \sum_i x_{k-1,i}^2
\end{bmatrix} \hspace{5pt},\hspace{5pt} X'Y =
\begin{bmatrix}
\sum_i y_i \\
\sum_i x_{1i}y_i \\
\sum_i x_{2i}y_i \\
\vdots \\
\sum_i x_{k-1,i}y_i
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="ejemplo">
<h3>2.12.2 Ejemplo<a class="headerlink" href="#ejemplo" title="Link to this heading">#</a></h3>
<p>Usando los siguientes datos queremos estimar el modelo <span class="math notranslate nohighlight">\(Y = X\beta + \varepsilon\)</span>, donde <span class="math notranslate nohighlight">\(\beta = [\beta_0,  \beta_1]'\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y =
\begin{bmatrix}
8 \\
9 \\
4 \\
2 \\
7 \\
3 \\
\end{bmatrix} \hspace{2pt},\hspace{2pt} X =
\begin{bmatrix}
1 &amp; 4 \\
1 &amp; 3 \\
1 &amp; 10 \\
1 &amp; 11 \\
1 &amp; 6 \\
1 &amp; 9 \\
\end{bmatrix}
\end{split}\]</div>
<p>A partir de esta información debemos calcular las siguientes matrices:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X'X =
\begin{bmatrix}
6 &amp; 43 \\
43 &amp; 363 \\
\end{bmatrix} \hspace{2pt},\hspace{2pt} (X'X)^{-1} = \frac{1}{329} \cdot
\begin{bmatrix}
363 &amp; -43 \\
-43 &amp; 6 \\
\end{bmatrix} \hspace{2pt},\hspace{2pt} X'Y =
\begin{bmatrix}
33 \\
190 \\
\end{bmatrix}
\end{split}\]</div>
<p>Recuerde que nuestro estimador de <span class="math notranslate nohighlight">\(\beta\)</span> es <span class="math notranslate nohighlight">\(\hat \beta = (X'X)^{-1}X'Y\)</span>. Así,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat\beta = \frac{1}{329} \cdot
\begin{bmatrix}
363 &amp; -43 \\
-43 &amp; 6 \\
\end{bmatrix} \cdot
\begin{bmatrix}
33 \\
190 \\
\end{bmatrix} =
\begin{bmatrix}
11.5775076 \\
-0.8480243 \\
\end{bmatrix}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
<span class="n">x1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span>
<span class="n">x2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">11</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">)</span>
<span class="n">X</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">cbind</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">)</span>

<span class="n">XX</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">%*%</span><span class="n">X</span>
<span class="n">XX1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">solve</span><span class="p">(</span><span class="n">XX</span><span class="p">)</span>
<span class="n">XY</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">%*%</span><span class="n">Y</span>

<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">XX1</span><span class="o">%*%</span><span class="n">XY</span><span class="p">;</span><span class="w"> </span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A matrix: 2 × 1 of type dbl</caption>
<tbody>
	<tr><th scope=row>x1</th><td>11.5775076</td></tr>
	<tr><th scope=row>x2</th><td>-0.8480243</td></tr>
</tbody>
</table>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">Y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Call:
lm(formula = Y ~ x2)

Residuals:
       1        2        3        4        5        6 
-0.18541 -0.03343  0.90274 -0.24924  0.51064 -0.94529 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 11.57751    0.75506  15.333 0.000106 ***
x2          -0.84802    0.09707  -8.736 0.000946 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7188 on 4 degrees of freedom
Multiple R-squared:  0.9502,	Adjusted R-squared:  0.9377 
F-statistic: 76.31 on 1 and 4 DF,  p-value: 0.0009461
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="propiedades-de-hat-beta">
<h2>2.13 Propiedades de <span class="math notranslate nohighlight">\(\hat\beta\)</span><a class="headerlink" href="#propiedades-de-hat-beta" title="Link to this heading">#</a></h2>
<p>Mostremos ahora <span class="math notranslate nohighlight">\(E(\hat\beta) = \beta\)</span>. Observe que</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat\beta &amp;= (X'X)^{-1}X'Y \\
&amp;= (X'X)^{-1}X'(X\beta + \varepsilon) \\
&amp;= (X'X)^{-1}X'X\beta + (X'X)^{-1}X'\varepsilon) \\
&amp;= \beta + (X'X)^{-1}X'\varepsilon \\
\end{align*}\]</div>
<p>Luego, es fácil ver que</p>
<div class="math notranslate nohighlight">
\[E(\hat\beta | X) = \beta + (X'X)^{-1}X'E(\varepsilon | X)\]</div>
<p>Por el supuesto de independencia condicional <span class="math notranslate nohighlight">\(E(\varepsilon | X) = 0\)</span>. Lo que implica que <span class="math notranslate nohighlight">\(E(\hat\beta | X) = \beta\)</span></p>
<p>Por la ley de esperanzas iteradas podemos demostrar que</p>
<div class="math notranslate nohighlight">
\[E[E(\hat\beta | X)] = E[\hat\beta] = \beta\]</div>
</section>
<section id="matriz-de-varianza-y-covarianza">
<h2>2.14 Matriz de Varianza y Covarianza<a class="headerlink" href="#matriz-de-varianza-y-covarianza" title="Link to this heading">#</a></h2>
<p>Ahora queremos determinar la varianza de <span class="math notranslate nohighlight">\(\hat\beta\)</span>, en forma matricial procedemos de la siguiente manera:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
V(\hat{\beta}|X ) &amp;= E \big[(\hat{\beta} - E[\hat{\beta}])(\hat{\beta} - E[\hat{\beta}])' \,|\, X \big] \\
&amp;= E \big[(\beta + (X'X)^{-1}X'\varepsilon - \beta])(\beta + (X'X)^{-1}X'\varepsilon - \beta])' \,|\, X \big] \\
&amp;= E \big[((X'X)^{-1}X'\varepsilon)((X'X)^{-1}X'\varepsilon)' \,|\, X \big] \\
&amp;= E \big[(X'X)^{-1}X'\varepsilon\varepsilon'X(X'X)^{-1} \,|\, X \big] \\
&amp;= (X'X)^{-1}X' E[\varepsilon\varepsilon'|X] X (X'X)^{-1} \\
&amp;= (X'X)^{-1}X' \sigma^2 I X (X'X)^{-1} \\
&amp;=\sigma^2 (X'X)^{-1}X'X (X'X)^{-1} \\
&amp;= \sigma^2 (X'X)^{-1}
\end{align*}\]</div>
<p>Observe que la solución anterior no solo nos permite encontrar las varianzas para cada <span class="math notranslate nohighlight">\(\hat\beta_j\)</span>, <span class="math notranslate nohighlight">\(j \in \{0, 1, ..., k-1\}\)</span>, sino también la covarianza entre <span class="math notranslate nohighlight">\(\hat\beta_j\)</span> y <span class="math notranslate nohighlight">\(\hat\beta_l\)</span> para todo <span class="math notranslate nohighlight">\(j \neq l\)</span>. Es decir,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V(\hat{\beta} | X) =
\begin{bmatrix}
Var(\hat{\beta}_0) &amp; Cov(\hat{\beta}_0, \hat{\beta}_1) &amp; \dots &amp; Cov(\hat{\beta}_0, \hat{\beta}_K) \\
Cov(\hat{\beta}_1, \hat{\beta}_0) &amp; Var(\hat{\beta}_1) &amp; \dots &amp; Cov(\hat{\beta}_1, \hat{\beta}_K) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
Cov(\hat{\beta}_K, \hat{\beta}_0) &amp; Cov(\hat{\beta}_K, \hat{\beta}_1) &amp; \dots &amp; Var(\hat{\beta}_K)
\end{bmatrix} = \sigma^2 (X'X)^{-1}
\end{split}\]</div>
<p>Recuerde que un estimador para <span class="math notranslate nohighlight">\(\sigma^2\)</span> es:</p>
<div class="math notranslate nohighlight">
\[\hat\sigma^2 = \frac{\sum_i e_i^2}{n-k} = \frac{e'e}{n-k}\]</div>
<p>Entonces,</p>
<div class="math notranslate nohighlight">
\[\widehat{V(\hat{\beta} | X)} = \frac{e'e}{n-k}(X'X)^{-1}\]</div>
</section>
<section id="pruebas-de-significancia-individual">
<h2>2.15 Pruebas de Significancia Individual<a class="headerlink" href="#pruebas-de-significancia-individual" title="Link to this heading">#</a></h2>
<p>Para evaluar nuestro modelo, podemos considerar pruebas de hipótesis individuales para cada uno de los parámetros desconocidos <span class="math notranslate nohighlight">\(\beta_j\)</span>. Es decir, para todo <span class="math notranslate nohighlight">\(j \in \{0, ..., k-1\}\)</span> evaluamos:</p>
<div class="math notranslate nohighlight">
\[H_0 : \beta_j = 0\]</div>
<div class="math notranslate nohighlight">
\[H_1 : \beta_j \neq 0\]</div>
<p>Ya que asumimos que <span class="math notranslate nohighlight">\(\varepsilon  \sim N\left(0, \sigma^2 I\right)\)</span>, nuestros estimadores <span class="math notranslate nohighlight">\(\hat\beta \sim N\left(\beta, \sigma^2(X'X)^{-1}\right)\)</span>. Así, para evaluar nuestra hipótesis podemos usar el estadístico de prueba:</p>
<div class="math notranslate nohighlight">
\[T = \frac{\hat\beta_j}{\text{ee}(\hat\beta_j)} \sim t_{n-k}\]</div>
<p>Observe que <span class="math notranslate nohighlight">\(\text{ee}(\hat\beta_j)\)</span> se puede computar a partir de la matriz <span class="math notranslate nohighlight">\(\hat\sigma^2 (X'X)^{-1}\)</span>.</p>
<p>En nuestro ejemplo anterior</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y = [8, 9, 4, 2, 7, 3]' \hspace{5pt},\hspace{5pt}
(X'X)^{-1} = \frac{1}{329} \cdot
\begin{bmatrix}
363 &amp; -43 \\
-43 &amp; 6 \\
\end{bmatrix} \hspace{5pt},\hspace{5pt} \hat\beta =
\begin{bmatrix}
11.58 \\
-0.85 \\
\end{bmatrix}
\end{split}\]</div>
<p>Usando esta información podemos calcular:</p>
<div class="math notranslate nohighlight">
\[\hat Y = X\hat\beta = [8.18, 9.03, 3.10, 2.25, 6.49, 3.94]\]</div>
<div class="math notranslate nohighlight">
\[e = Y - \hat Y = [-0.18, -0.03, 0.90, -0.25, 0.51, -0.94]\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\hat\sigma^2=\frac{e'e}{6-2} = 0.516 \hspace{5pt},\hspace{5pt} \widehat{V(\hat\beta)} = \hat\sigma^2 \cdot (X'X)^{-1} = \frac{1}{329} \cdot
\begin{bmatrix}
363 &amp; -43 \\
-43 &amp; 6 \\
\end{bmatrix} =
\begin{bmatrix}
0.570 &amp; -0.0675 \\
-0.0675 &amp; 0.009 \\
\end{bmatrix}\end{split}\]</div>
<p>Luego, ee<span class="math notranslate nohighlight">\((\beta_0) = \sqrt{0.570} = 0.7551\)</span>, ee<span class="math notranslate nohighlight">\((\beta_1) = \sqrt{0.009} = 0.0971\)</span>, <span class="math notranslate nohighlight">\(Cov(\beta_0, \beta_1) = -0.0675\)</span></p>
</section>
<section id="bondad-de-ajuste-r-2-y-r-2-ajustado">
<h2>2.16 Bondad de Ajuste: <span class="math notranslate nohighlight">\(R^2\)</span> y <span class="math notranslate nohighlight">\(R^2\)</span>-Ajustado<a class="headerlink" href="#bondad-de-ajuste-r-2-y-r-2-ajustado" title="Link to this heading">#</a></h2>
<p>Al igual que en el caso de regresión simple, definimos</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
R^2 &amp;= \frac{\sum_i (\hat y_i -\bar{y})^2}{\sum_i (y_i -\bar{y})^2} = 1 - \frac{\sum_i (y_i - \hat y_i)^2}{\sum_i (y_i -\bar{y})^2} \\
&amp;= \frac{SSR}{SST} = 1 - \frac{SSE}{SST} 
\end{align*}\]</div>
<p>Sin embargo, <span class="math notranslate nohighlight">\(R^2\)</span> <strong>aumentará siempre que incluyamos una covariable adicional</strong>. Incluso si dicha covariable es ‘ruido’.</p>
<p>En el caso de regresión múltiple definimos un coeficiente ‘penaliza’ el poder predictivo del modelo cuando incluimos variables irrelevantes:</p>
<div class="math notranslate nohighlight">
\[R^2\text{- ajustado} = 1 - \frac{SSE\hspace{2pt}/\hspace{2pt}(n-k)}{SST\hspace{2pt}/\hspace{2pt}(n-1)}\]</div>
<p>Observe que al incluir una variable irrelevante, los residuales de nuestro modelo <span class="math notranslate nohighlight">\(e_i = y_i - \hat y_i\)</span> no cambiarán de manera considerable (con respecto al modelo que no incluye ruido). Sin embargo, ahora estamos dividiendo <span class="math notranslate nohighlight">\(SSE\)</span> por <span class="math notranslate nohighlight">\(n-k\)</span> (que penaliza el número de parámetros que queremos estimar).</p>
<p>Así, mientras <span class="math notranslate nohighlight">\(R^2\)</span> aumenta con el número de variables, <span class="math notranslate nohighlight">\(R^2\)</span>-ajustado puede disminuir. Veamos un ejemplo:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">educ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exper</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">expersq</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wage1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="n">Error</span> <span class="ow">in</span> <span class="nb">eval</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span> <span class="n">parent</span><span class="o">.</span><span class="n">frame</span><span class="p">()):</span> <span class="nb">object</span> <span class="s1">&#39;wage1&#39;</span> <span class="ow">not</span> <span class="n">found</span>
<span class="ne">Traceback</span>:

<span class="mi">1</span><span class="o">.</span> <span class="n">lm</span><span class="p">(</span><span class="n">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span> <span class="o">~</span> <span class="n">educ</span> <span class="o">+</span> <span class="n">exper</span> <span class="o">+</span> <span class="n">expersq</span> <span class="o">+</span> <span class="n">female</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">wage1</span><span class="p">)</span>
<span class="mi">2</span><span class="o">.</span> <span class="nb">eval</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span> <span class="n">parent</span><span class="o">.</span><span class="n">frame</span><span class="p">())</span>
<span class="mi">3</span><span class="o">.</span> <span class="nb">eval</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span> <span class="n">parent</span><span class="o">.</span><span class="n">frame</span><span class="p">())</span>
<span class="mi">4</span><span class="o">.</span> <span class="n">stats</span><span class="p">::</span><span class="n">model</span><span class="o">.</span><span class="n">frame</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span> <span class="o">~</span> <span class="n">educ</span> <span class="o">+</span> <span class="n">exper</span> <span class="o">+</span> <span class="n">expersq</span> <span class="o">+</span> 
 <span class="o">.</span>     <span class="n">female</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">wage1</span><span class="p">,</span> <span class="n">drop</span><span class="o">.</span><span class="n">unused</span><span class="o">.</span><span class="n">levels</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">)</span>
<span class="mi">5</span><span class="o">.</span> <span class="n">model</span><span class="o">.</span><span class="n">frame</span><span class="o">.</span><span class="n">default</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span> <span class="o">~</span> <span class="n">educ</span> <span class="o">+</span> <span class="n">exper</span> <span class="o">+</span> <span class="n">expersq</span> <span class="o">+</span> 
 <span class="o">.</span>     <span class="n">female</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">wage1</span><span class="p">,</span> <span class="n">drop</span><span class="o">.</span><span class="n">unused</span><span class="o">.</span><span class="n">levels</span> <span class="o">=</span> <span class="n">TRUE</span><span class="p">)</span>
<span class="mi">6</span><span class="o">.</span> <span class="ow">is</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">frame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="mi">7</span><span class="o">.</span> <span class="o">.</span><span class="n">handleSimpleError</span><span class="p">(</span><span class="n">function</span> <span class="p">(</span><span class="n">cnd</span><span class="p">)</span> 
 <span class="o">.</span> <span class="p">{</span>
 <span class="o">.</span>     <span class="n">watcher</span><span class="err">$</span><span class="n">capture_plot_and_output</span><span class="p">()</span>
 <span class="o">.</span>     <span class="n">cnd</span> <span class="o">&lt;-</span> <span class="n">sanitize_call</span><span class="p">(</span><span class="n">cnd</span><span class="p">)</span>
 <span class="o">.</span>     <span class="n">watcher</span><span class="err">$</span><span class="n">push</span><span class="p">(</span><span class="n">cnd</span><span class="p">)</span>
 <span class="o">.</span>     <span class="n">switch</span><span class="p">(</span><span class="n">on_error</span><span class="p">,</span> <span class="k">continue</span> <span class="o">=</span> <span class="n">invokeRestart</span><span class="p">(</span><span class="s2">&quot;eval_continue&quot;</span><span class="p">),</span> 
 <span class="o">.</span>         <span class="n">stop</span> <span class="o">=</span> <span class="n">invokeRestart</span><span class="p">(</span><span class="s2">&quot;eval_stop&quot;</span><span class="p">),</span> <span class="n">error</span> <span class="o">=</span> <span class="n">NULL</span><span class="p">)</span>
 <span class="o">.</span> <span class="p">},</span> <span class="s2">&quot;object &#39;wage1&#39; not found&quot;</span><span class="p">,</span> <span class="n">base</span><span class="p">::</span><span class="n">quote</span><span class="p">(</span><span class="nb">eval</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span> <span class="n">parent</span><span class="o">.</span><span class="n">frame</span><span class="p">())))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">wage1</span><span class="o">$</span><span class="n">wage</span><span class="p">)</span>
<span class="n">wage1</span><span class="o">$</span><span class="n">noise</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">educ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exper</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">expersq</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">noise</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wage1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="prueba-de-signifiancia-global">
<h2>2.17 Prueba de Signifiancia Global<a class="headerlink" href="#prueba-de-signifiancia-global" title="Link to this heading">#</a></h2>
<p>Al igual que en el caso de regresión simple, queremos evaluar el modelo de manera conjunta. Para ello consideramos la siguiente prueba de hipótesis:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1 = \beta_2 = ... = \beta_{k-1}\]</div>
<div class="math notranslate nohighlight">
\[H_1: \beta_j \neq 0 \text{ para cualquier } j \in \{1, ..., k-1\}\]</div>
<p>Nuestro estadístico de prueba para evaluar esta hipótesis es:</p>
<div class="math notranslate nohighlight">
\[F = \frac{MSR}{MSE} = \frac{SSR/(k-1)}{SSE/(n-k)} = \frac{\sum_i(\hat y_i - \bar{y})^2/(k-1)}{\sum_i(y_i - \hat y_i)^2/(n-k)} \sim F_{k-1,n-k}\]</div>
</section>
<section id="otras-pruebas-de-hipotesis">
<h2>2.18 Otras Pruebas de Hipotesis<a class="headerlink" href="#otras-pruebas-de-hipotesis" title="Link to this heading">#</a></h2>
<p>En el modelo multivariado podemos evaluar hipótesis para cuaquier (sub-)conjunto de parámetros. Algunos ejemplos son:</p>
<p>Sean <span class="math notranslate nohighlight">\(j, l, t \in \{0, 1, ..., k-1\}\)</span> y <span class="math notranslate nohighlight">\(c_1, c_2, c_3 \in \mathbb{R}\)</span>,</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_j = \beta_{l} + c_1\]</div>
<div class="math notranslate nohighlight">
\[H_0: \beta_j + \beta_{l} = \beta_{t}\]</div>
<div class="math notranslate nohighlight">
\[H_0: \beta_j = c_1\beta_l + c_2 \beta_t + c_3\]</div>
<p>A continuación presentamos dos estadísticos de prueba con los cuales podemos evaluar nuestras hipótesis.</p>
<section id="estadistico-t">
<h3>2.18.1 Estadístico T<a class="headerlink" href="#estadistico-t" title="Link to this heading">#</a></h3>
<p>Para la prueba</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_j = \beta_{l} + c_1\]</div>
<div class="math notranslate nohighlight">
\[H_1: \beta_j \neq \beta_{l} + c_1\]</div>
<p>Podemos usar el estadístico</p>
<div class="math notranslate nohighlight">
\[T = \frac{\hat\beta_j - \hat\beta_l - c_1}{\text{ee}(\hat\beta_j - \hat\beta_l - c_1)} = \frac{\hat\beta_j - \hat\beta_l - c_1}{\sqrt{Var(\hat\beta_j) + Var(\hat\beta_l) - 2Cov(\hat\beta_j, \hat\beta_l)}} \sim t_{n-k}\]</div>
<p>Observe que el error estandar de esta prueba lo podemos calcular a partir de la matrix de varianza-covarianza: <span class="math notranslate nohighlight">\(V(\hat\beta)\)</span>.</p>
</section>
</section>
<section id="minimos-cuadrados-restringidos">
<h2>2.19 Minimos Cuadrados Restringidos<a class="headerlink" href="#minimos-cuadrados-restringidos" title="Link to this heading">#</a></h2>
<p>Este tipo de pruebas también se pueden hacer usando una regresión auxiliar a la cual se aplica la restricción.</p>
<p>Suponga que tenemos el siguiente modelo:</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} + \varepsilon_i
\]</div>
<p>Nos interesa evaluar:</p>
<div class="math notranslate nohighlight">
\[
H_0 : \beta_1 = \beta_3 = 0
\]</div>
<p>Entonces el modelo restringido sería:</p>
<div class="math notranslate nohighlight">
\[
y_i = \beta_0 + \beta_2 x_{2i} + \beta_4 x_{4i} + \varepsilon_i
\]</div>
<p>Hay varias formas de evaluar estas pruebas sin usar la matriz <span class="math notranslate nohighlight">\(R\)</span>.</p>
<p>Sin embargo, todas requieren de una regresión auxiliar donde usemos la restricción.</p>
<p>Los estadísticos que podemos usar son:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
F_{J,n-K} &amp;= \frac{R^2_{NR} - R^2_{R}}{J} \Big/ \frac{(1 - R^2_{NR})}{n-k} \\
&amp;= \frac{SSE_{R} - SSE_{NR}}{J} \Big/ \frac{SSE_{NR}}{n-k} \\
&amp;= \frac{e'_{R}e_{R} - e'_{NR}e_{NR}}{J} \Big/ \frac{e'_{NR}e_{NR}}{n-k} \\
\end{align*}\]</div>
<p>Todas estas son fórmulas equivalentes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Modelo No Restringido</span>
<span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">Y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x3</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Modelo Restringido</span>
<span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">Y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="id1">
<h3>2.19.1 Ejemplo<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Calculemos el estadístico F asociado a la prueba <span class="math notranslate nohighlight">\(H_0: \beta_2 = 0\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
F_{J,n-K} &amp;= \frac{R^2_{NR} - R^2_{R}}{J} \Big/ \frac{(1 - R^2_{NR})}{n-k} \\
&amp;= \frac{0.9521 - 0.9502}{1} \Big/ \frac{(1 - 0.9521)}{3} \approx 0.12
\end{align*}\]</div>
<p>Tambien lo podemos calcular de la siguiente manera:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
F_{J,n-K} &amp;= \frac{SSE_{R} - SSE_{NR}}{J} \Big/ \frac{SSE_{NR}}{n-k} \\ \\
&amp;= \frac{2.067 - 1.987}{1} \Big/ \frac{(1.987)}{3} \approx 0.12
\end{align*}\]</div>
<p>Observe que:</p>
<div class="math notranslate nohighlight">
\[SSE_{NR} = MSE_{NR}*(n-k) = 0.8138^2 * (6-3) = 1.987\]</div>
<div class="math notranslate nohighlight">
\[SSE_{R} = MSE_{R}*(n-k) = 0.7188^2 * (6-2) = 2.067\]</div>
</section>
<section id="seleccion-de-variables">
<h3>2.19.2 Selección de Variables<a class="headerlink" href="#seleccion-de-variables" title="Link to this heading">#</a></h3>
<p>Este método es útil para saber qué variables deben entrar y cuáles no en un modelo.</p>
<ol class="arabic simple">
<li><p>Corremos el modelo completo.</p></li>
<li><p>Identificamos las variables que no son significativas o que tienen coeficientes raros.</p></li>
<li><p>Excluimos esas variables, y corremos el modelo sin ellas.</p></li>
<li><p>Hacemos una prueba F tal que:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
H_0 : \beta_{exc} = 0
\]</div>
<p><span class="math notranslate nohighlight">\(\hspace{15pt}\)</span> donde <span class="math notranslate nohighlight">\(\beta_{exc}\)</span> son los betas excluidos.</p>
<ol class="arabic simple" start="5">
<li><p>Si se rechaza <span class="math notranslate nohighlight">\(H_0\)</span>, entonces ese grupo de variables se debe dejar.</p></li>
<li><p>De lo contrario, las podemos sacar del modelo.</p></li>
</ol>
</section>
</section>
<section id="variables-binarias-y-categoricas">
<h2>2.20 Variables Binarias y Categoricas<a class="headerlink" href="#variables-binarias-y-categoricas" title="Link to this heading">#</a></h2>
<p>Hay muchas variables que no podemos incluir linealmente a un modelo porque son cualitativas. Sin embargo, podemos incluir variables categóricas.</p>
<p>Ejemplos: género, recibe subsidio o no, máximo nivel educativo, etc.</p>
<p>Para ello creamos variables binarias (tambien conocidas como variables <em>dummy</em>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
D_i = 1(\text{Genero}_i = \text{mujer}) = 
\begin{cases} 
1 &amp; \text{si mujer} \\ 
0 &amp; \text{si hombre} 
\end{cases}
\end{split}\]</div>
<p>En nuestra matrix <span class="math notranslate nohighlight">\(X\)</span> estas variables se representan a traves de vectores de 1 y 0. Por ejemplo:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X =
\begin{bmatrix}
1 &amp; 0 &amp; 4 \\
1 &amp; 1 &amp; 5.6 \\
1 &amp; 0 &amp; 8.9 \\
1 &amp; 1 &amp; 3.2 \\
\end{bmatrix}
\end{split}\]</div>
<p>Las variables categóricas son fáciles de incluir en un modelo.</p>
<p>Para interpretar estas variables debemos considerar la <strong>categoría excluida</strong>.</p>
<ul class="simple">
<li><p>Si hay <span class="math notranslate nohighlight">\(m\)</span> categorías, entonces podemos incluir solo <span class="math notranslate nohighlight">\(m - 1\)</span> variables binarias.</p></li>
<li><p>De lo contrario, habría <strong>colinealidad perfecta</strong> (con el vector de 1s asociado al intercepto del modelo) y el modelo no se podría estimar.</p></li>
</ul>
</section>
<section id="aplicacion">
<h2>2.21 Aplicación<a class="headerlink" href="#aplicacion" title="Link to this heading">#</a></h2>
<p>Consideremos la siguiente extensión del modelo minceriano:</p>
<div class="math notranslate nohighlight">
\[\log(w_i) = \beta_0 + \beta_1 s_i + \beta_2 x_i + \beta_3 x_i^2 + \beta_4 f_i + \sum_{j=2}^4 \mu_j 1(\text{Occup}_i = j) + \varepsilon_i\]</div>
<p>Donde <span class="math notranslate nohighlight">\(\text{Occup}_i\)</span> es la ocupacion del individuo <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Observe que la sumatoria no inicia en la categoria 1 porque debemos fijar una <strong>grupo de comparación</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">wage1</span><span class="o">$</span><span class="n">occup</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="p">;</span><span class="w"> </span><span class="n">wage1</span><span class="o">$</span><span class="n">occup</span><span class="p">[</span><span class="n">wage1</span><span class="o">$</span><span class="n">profocc</span><span class="o">==</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="p">;</span><span class="w"> </span><span class="n">wage1</span><span class="o">$</span><span class="n">occup</span><span class="p">[</span><span class="n">wage1</span><span class="o">$</span><span class="n">clerocc</span><span class="o">==</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">3</span><span class="p">;</span><span class="w"> </span><span class="n">wage1</span><span class="o">$</span><span class="n">occup</span><span class="p">[</span><span class="n">wage1</span><span class="o">$</span><span class="n">servocc</span><span class="o">==</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">4</span>
<span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">educ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exper</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">expersq</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">I</span><span class="p">(</span><span class="n">occup</span><span class="o">==</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">I</span><span class="p">(</span><span class="n">occup</span><span class="o">==</span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">I</span><span class="p">(</span><span class="n">occup</span><span class="o">==</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">I</span><span class="p">(</span><span class="n">occup</span><span class="o">==</span><span class="m">1</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wage1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">educ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exper</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">expersq</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">factor</span><span class="p">(</span><span class="n">occup</span><span class="p">),</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wage1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="interacciones">
<h2>2.22 Interacciones<a class="headerlink" href="#interacciones" title="Link to this heading">#</a></h2>
<p>Las variables binarias son una muy buena herramienta.</p>
<ul class="simple">
<li><p>Una aplicación muy útil son las <strong>interacciones</strong>.</p></li>
<li><p>Una interacción es simplemente el <strong>producto de una variable</strong> (categórica o no) con una variable categórica.</p></li>
<li><p>Las interacciones permiten medir <strong>cambios en pendiente</strong> y diferenciar grupos por varias características.</p></li>
</ul>
<p>Considere el siguiente modelo:</p>
<div class="math notranslate nohighlight">
\[\log(w_i) = \beta_0 + \beta_1 f_i + \beta_2 c_i + \beta_3 s_i + \varepsilon_i\]</div>
<p>Donde</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_i\)</span> es el salario del individuo <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(s_i\)</span> los años de educación</p></li>
<li><p><span class="math notranslate nohighlight">\(f_i\)</span> toma el valor de 1 si es mujer, y <span class="math notranslate nohighlight">\(c_i\)</span> toma el valor de 1 si está casado (o casada).</p></li>
</ul>
<section id="interaccion-entre-variables-binarias">
<h3>2.22.1 Interacción entre Variables Binarias<a class="headerlink" href="#interaccion-entre-variables-binarias" title="Link to this heading">#</a></h3>
<p>La interacción entre <span class="math notranslate nohighlight">\(f_i\)</span> y <span class="math notranslate nohighlight">\(c_i\)</span> nos permitirá diferenciar las <strong>mujeres casadas</strong>.</p>
<div class="math notranslate nohighlight">
\[\log(w_i) = \beta_0 + \beta_1 f_i + \beta_2 c_i + \beta_3 s_i + \delta (f_i \times c_i) + \varepsilon_i\]</div>
<p>Observe que:</p>
<div class="math notranslate nohighlight">
\[\Delta_{f_i=0}(c_i)  = E[\log(w_i) \mid f_i = 0, c_i = 1] - E[\log(w_i) \mid f_i = 0, c_i = 0] = \beta_2\]</div>
<div class="math notranslate nohighlight">
\[\Delta_{f_i=1}(c_i) = E[\log(w_i) \mid f_i = 1, c_i = 1] - E[\log(w_i) \mid f_i = 1, c_i = 0] = \beta_2 + \delta\]</div>
<p>Entonces,</p>
<div class="math notranslate nohighlight">
\[\delta = \Delta_{f_i=1}(c_i) - \Delta_{f_i=0}(c_i)\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span>: cambio en el salario para hombres casados (vs hombres solteros)</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span> + <span class="math notranslate nohighlight">\(\delta\)</span>: cambio en el salario para mujeres casadas (vs mujeres solteras)</p></li>
<li><p><span class="math notranslate nohighlight">\(\delta\)</span>: cambio diferencial en el salario para mujeres casadas (vs al cambio para hombres casados)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">married</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">educ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="o">*</span><span class="n">married</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wage1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="interaccion-entre-una-variable-binaria-y-una-continua">
<h3>2.22.2 Interacción entre una Variable Binaria y una Continua<a class="headerlink" href="#interaccion-entre-una-variable-binaria-y-una-continua" title="Link to this heading">#</a></h3>
<p>Considere ahora la interacción entre una variable categórica y una variable continua.</p>
<p>Esto identifica <strong>cambios en pendientes</strong> o efectos diferenciados.</p>
<div class="math notranslate nohighlight">
\[\log(w_i) = \beta_0 + \beta_1 f_i + \beta_2 s_i + \beta_3 (f_i \times s_i) + \varepsilon_i\]</div>
<p>Podemos calcular la derivada:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \log(w_i)}{\partial s_i} = \beta_2 + \beta_3 f_i
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_2\)</span>: efecto de la educación en el salario de los <strong>hombres</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_2 + \beta_3\)</span>: efecto de la educación en el salario promedio de las <strong>mujeres</strong>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="nf">log</span><span class="p">(</span><span class="n">wage</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">female</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">educ</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">female</span><span class="o">*</span><span class="n">educ</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wage1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="teorema-de-gauss-markov">
<h2>2.23 Teorema de Gauss-Markov<a class="headerlink" href="#teorema-de-gauss-markov" title="Link to this heading">#</a></h2>
<div class="alert alert-block alert-info">
<b>TEOREMA:</b>
<p>
<p>El estimador de Mínimos Cuadrados Ordinarios <span class="math notranslate nohighlight">\(\hat{\beta} = (X'X)^{-1}X'Y\)</span> es el <strong>mejor estimador lineal insesgado (MELI)</strong>.</p>
</div>
<ul class="simple">
<li><p>Esto significa que es el estimador lineal más <strong>eficiente</strong> o con menor varianza</p></li>
<li><p>Esto ocurre <strong>si y solo si</strong> se cumplen los supuestos del modelo lineal</p></li>
</ul>
<section id="demostacion-del-teorema-de-gauss-markov">
<h3>2.23.1 Demostación del Teorema de Gauss-Markov<a class="headerlink" href="#demostacion-del-teorema-de-gauss-markov" title="Link to this heading">#</a></h3>
<p>Considere un estimador lineal insesgado <span class="math notranslate nohighlight">\(b\)</span>, tal que:</p>
<div class="math notranslate nohighlight">
\[b = CY \quad \text{y} \quad C \neq (X'X)^{-1}X'\]</div>
<p>De forma genérica podemos definir:</p>
<div class="math notranslate nohighlight">
\[
C = (D + A) \quad \text{tal que} \quad A = (X'X)^{-1}X'
\]</div>
<p>Entonces:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
b &amp;= CY = DY + AY \\
&amp;= D(X\beta + \varepsilon) + \hat{\beta}
\end{align*}\]</div>
<p>Como <span class="math notranslate nohighlight">\(b\)</span> es insesgado:</p>
<div class="math notranslate nohighlight">
\[
E[b|X] = \beta
\]</div>
<div class="math notranslate nohighlight">
\[
E[b|X] = DX\beta + D \cdot E[\varepsilon|X] + E[\hat{\beta}|X]
\]</div>
<p>Entonces debe cumplirse que:</p>
<div class="math notranslate nohighlight">
\[
DX = 0
\]</div>
<p>Podemos entonces escribir lo siguiente:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
b &amp;= DX\beta + D\varepsilon + \hat{\beta} \\
&amp;= D\varepsilon + \beta + A\varepsilon \\
&amp;= \beta + (D + A)\varepsilon
\end{align*}\]</div>
<p>Ahora saquemos la varianza de este estimador:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Var(b) = (D + A)Var(\varepsilon)(D + A)' \\
&amp;= \sigma^2 (D + A)(D + A)' \\
&amp;= \sigma^2[DD' + DA' + AD' + AA'] \\
&amp;= \sigma^2DD' + \sigma^2(X'X)^{-1}\\
\end{align*}\]</div>
<p>Noten que <span class="math notranslate nohighlight">\(DD'\)</span> es un valor positivo.</p>
<p>Por lo tanto:</p>
<div class="math notranslate nohighlight">
\[
Var(b) \geq Var(\hat{\beta}) = \sigma^2 (X'X)^{-1}
\]</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Modulo_1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Parte I - Fundamentos</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-aprendizaje-y-tamano-de-clase">2.1 Aplicación: Aprendizaje y Tamaño de Clase</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regresion-simple-o-univariada">2.2 Regresión Simple o Univariada</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supuestos-del-modelo">2.3 Supuestos del Modelo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linealidad-en-parametros">2.3.1 Linealidad en Parámetros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-multicolinealidad-o-rango-completo">2.3.2 No Multicolinealidad o Rango Completo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homoscedasticidad">2.3.3 Homoscedasticidad</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-autocorrelacion">2.3.4 No Autocorrelación</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distribucion-normal-de-los-errores">2.3.5 Distribución Normal de los Errores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independencia-condicional-o-exogenidad">2.3.6 Independencia Condicional o Exogenidad</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimacion-por-mco">2.4 Estimación por MCO</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion-relacion-entre-aprendizaje-y-crecimiento">2.5 Aplicacion : Relación entre Aprendizaje y Crecimiento</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residuales-de-la-regresion-e-i-y-i-hat-y-i">2.6 Residuales de la Regresión: <span class="math notranslate nohighlight">\(e_i = Y_i - \hat Y_i\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propiedades-de-los-estimadores-hat-beta-k">2.7 Propiedades de los estimadores <span class="math notranslate nohighlight">\(\hat\beta_k\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inferencia-en-regresion-simple">2.8 Inferencia en Regresión Simple</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruebas-de-hipotesis-sobre-hat-beta-k">2.8.1 Pruebas de Hipótesis sobre <span class="math notranslate nohighlight">\(\hat\beta_k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#varianza-de-hat-beta-k">2.8.2 Varianza de <span class="math notranslate nohighlight">\(\hat\beta_k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#varianza-de-los-errores">2.8.3 Varianza de los Errores</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruebas-de-hipotesis-individuales">2.8.4 Pruebas de Hipótesis Individuales</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intervalo-de-confianza-para-beta-k">2.8.4 Intervalo de confianza para <span class="math notranslate nohighlight">\(\beta_k\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coeficiente-de-determinacion-r-2">2.8.5 Coeficiente de Determinación: <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prueba-de-significancia-global">2.8.6 Prueba de Significancia Global</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion-e-inferencia-para-hat-y-i">2.8.7 Predicción e Inferencia para <span class="math notranslate nohighlight">\(\hat Y_i\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion-media">2.8.8 Predicción Media</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediccion-individual">2.8.9 Predicción Individual</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimacion-por-maxima-verosimilitud">2.9 Estimación por Máxima Verosimilitud</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-de-regresion-multiple">2.10 Modelo de Regresión Multiple</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modelo-en-forma-matricial">2.10.1 Modelo en Forma Matricial</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supuestos-en-forma-matricial">2.10.2 Supuestos en forma Matricial</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimos-cuadrados-ordinarios">2.11 Mínimos Cuadrados Ordinarios</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimacion-de-beta">2.12 Estimación de <span class="math notranslate nohighlight">\(\beta\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-y-sumatorias">2.12.1 Matrices y Sumatorias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo">2.12.2 Ejemplo</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#propiedades-de-hat-beta">2.13 Propiedades de <span class="math notranslate nohighlight">\(\hat\beta\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matriz-de-varianza-y-covarianza">2.14 Matriz de Varianza y Covarianza</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pruebas-de-significancia-individual">2.15 Pruebas de Significancia Individual</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bondad-de-ajuste-r-2-y-r-2-ajustado">2.16 Bondad de Ajuste: <span class="math notranslate nohighlight">\(R^2\)</span> y <span class="math notranslate nohighlight">\(R^2\)</span>-Ajustado</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prueba-de-signifiancia-global">2.17 Prueba de Signifiancia Global</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#otras-pruebas-de-hipotesis">2.18 Otras Pruebas de Hipotesis</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estadistico-t">2.18.1 Estadístico T</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#minimos-cuadrados-restringidos">2.19 Minimos Cuadrados Restringidos</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.19.1 Ejemplo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seleccion-de-variables">2.19.2 Selección de Variables</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variables-binarias-y-categoricas">2.20 Variables Binarias y Categoricas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#aplicacion">2.21 Aplicación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interacciones">2.22 Interacciones</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaccion-entre-variables-binarias">2.22.1 Interacción entre Variables Binarias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interaccion-entre-una-variable-binaria-y-una-continua">2.22.2 Interacción entre una Variable Binaria y una Continua</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#teorema-de-gauss-markov">2.23 Teorema de Gauss-Markov</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#demostacion-del-teorema-de-gauss-markov">2.23.1 Demostación del Teorema de Gauss-Markov</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sebastian Montano
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>